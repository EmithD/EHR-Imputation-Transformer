{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1052d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset_path = './data/physionet_39_features_and_target_no_leakage.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 're.admission.within.6.months'\n",
    "feature_cols = [col for col in df.columns if col != target_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeecd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].values\n",
    "y = df[target_var].values\n",
    "\n",
    "print(f\"Dataset loaded with {X.shape[0]} samples and {X.shape[1]} features\")\n",
    "print(f\"Target: {target_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d82c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].values\n",
    "y = df[target_var].values\n",
    "\n",
    "print(f\"Dataset loaded with {X.shape[0]} samples and {X.shape[1]} features\")\n",
    "print(f\"Target: {target_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00197877",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = ~np.isnan(X_train)\n",
    "mask_test = ~np.isnan(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_knn(X_missing, k=5):\n",
    "    \"\"\"Preimpute missing values using KNN imputer.\"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    X_imputed = imputer.fit_transform(X_missing)\n",
    "    return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing initial KNN imputation...\")\n",
    "X_train_knn_imputed = preprocess_with_knn(X_train)\n",
    "X_test_knn_imputed = preprocess_with_knn(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_knn_scaled = scaler.fit_transform(X_train_knn_imputed)\n",
    "X_test_knn_scaled = scaler.transform(X_test_knn_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNAA(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_norm=True):\n",
    "        \"\"\"\n",
    "        Improved Neighborhood Aware Autoencoder (I-NAA) as described in the paper.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_dim: Number of features in the dataset\n",
    "        - hidden_dim: Size of the hidden layer (undercomplete as per the paper)\n",
    "        - batch_norm: Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(ImprovedNAA, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        encoder_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        if batch_norm:\n",
    "            encoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        decoder_layers.append(nn.Linear(hidden_dim, input_dim))\n",
    "        if batch_norm:\n",
    "            decoder_layers.append(nn.BatchNorm1d(input_dim))\n",
    "        decoder_layers.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, output, target, mask):\n",
    "        \"\"\"\n",
    "        Custom loss function that only considers observed values in the loss calculation.\n",
    "        \n",
    "        Parameters:\n",
    "        - output: Model output\n",
    "        - target: True values\n",
    "        - mask: Binary mask where 1 indicates observed values and 0 indicates missing values\n",
    "        \"\"\"\n",
    "        mse_loss = self.mse(output, target)\n",
    "        masked_loss = mse_loss * mask\n",
    "        return masked_loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_knn_scaled),\n",
    "    torch.FloatTensor(X_train_knn_scaled),  # Target is the same as input for autoencoder\n",
    "    torch.FloatTensor(mask_train)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test_knn_scaled),\n",
    "    torch.FloatTensor(X_test_knn_scaled),  # Target is the same as input for autoencoder\n",
    "    torch.FloatTensor(mask_test)\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f40c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = input_dim // 2  # Undercomplete architecture as per paper\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "reapply_knn_every = 10  # Re-apply KNN every 10 epochs\n",
    "k_bounds = (3, 10)  # Range of k values for KNN\n",
    "\n",
    "model = ImprovedNAA(input_dim, hidden_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9471df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inaa(model, train_loader, val_loader, optimizer, criterion, num_epochs=100,\n",
    "              reapply_knn_every=10, k_bounds=(3, 10), device=device):\n",
    "    \"\"\"\n",
    "    Train the I-NAA model with the improvements described in the paper:\n",
    "    - Re-apply KNN imputation every N epochs with different k values\n",
    "    - Change the values to be imputed at each epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Get original data for re-imputation\n",
    "    X_train_np = X_train.copy()  # Original training data with missing values\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Re-apply KNN imputation with a different k value every N epochs\n",
    "        if epoch % reapply_knn_every == 0:\n",
    "            # Choose a random k value within the bounds\n",
    "            k = np.random.randint(k_bounds[0], k_bounds[1] + 1)\n",
    "            print(f\"Epoch {epoch}: Re-applying KNN imputation with k={k}\")\n",
    "            \n",
    "            X_train_reimputed = preprocess_with_knn(X_train_np, k=k)\n",
    "            X_train_reimputed_scaled = scaler.transform(X_train_reimputed)\n",
    "            \n",
    "            # Update the dataloader with re-imputed data\n",
    "            train_dataset = TensorDataset(\n",
    "                torch.FloatTensor(X_train_reimputed_scaled),\n",
    "                torch.FloatTensor(X_train_reimputed_scaled),\n",
    "                torch.FloatTensor(mask_train)\n",
    "            )\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, (data, target, mask) in enumerate(train_loader):\n",
    "            data, target, mask = data.to(device), target.to(device), mask.to(device)\n",
    "            \n",
    "            # Introduce random missingness for this epoch to change imputation locations\n",
    "            if epoch > 0:  # Skip for the first epoch to start with the original missingness\n",
    "                random_mask = torch.rand_like(mask) < 0.1  # 10% random new missingness\n",
    "                data_with_new_missingness = data.clone()\n",
    "                # Only apply to observed values (mask==1)\n",
    "                data_with_new_missingness[random_mask & (mask == 1)] = 0\n",
    "                # Update mask to reflect new missingness\n",
    "                new_mask = mask.clone()\n",
    "                new_mask[random_mask & (mask == 1)] = 0\n",
    "            else:\n",
    "                data_with_new_missingness = data\n",
    "                new_mask = mask\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data_with_new_missingness)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target, new_mask)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_data, val_target, val_mask in val_loader:\n",
    "                val_data, val_target, val_mask = val_data.to(device), val_target.to(device), val_mask.to(device)\n",
    "                val_output = model(val_data)\n",
    "                val_loss += criterion(val_output, val_target, val_mask).item()\n",
    "        \n",
    "        # Average loss\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_inaa_model.pt')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_inaa_model.pt'))\n",
    "    \n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining I-NAA model...\")\n",
    "model, train_losses, val_losses = train_inaa(\n",
    "    model, train_loader, val_loader, optimizer, criterion, num_epochs,\n",
    "    reapply_knn_every, k_bounds, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('I-NAA Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./models/inaa/inaa_training_progress.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93927fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete! Model saved as './models/inaa/best_inaa_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabdd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the I-NAA model class (same as in training)\n",
    "class ImprovedNAA(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_norm=True):\n",
    "        super(ImprovedNAA, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        encoder_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        if batch_norm:\n",
    "            encoder_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        decoder_layers.append(nn.Linear(hidden_dim, input_dim))\n",
    "        if batch_norm:\n",
    "            decoder_layers.append(nn.BatchNorm1d(input_dim))\n",
    "        decoder_layers.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45869323",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the original dataset...\")\n",
    "dataset_path = './data/physionet_39_features_and_target_no_leakage.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5309967",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 're.admission.within.6.months'\n",
    "feature_cols = [col for col in df.columns if col != target_var]\n",
    "X = df[feature_cols].values\n",
    "y = df[target_var].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ff642",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(X)\n",
    "\n",
    "# Check missing value statistics\n",
    "missing_counts = np.isnan(X).sum(axis=0)\n",
    "missing_percentage = np.isnan(X).sum() / X.size\n",
    "print(f\"Total missing values: {np.isnan(X).sum()} out of {X.size} ({missing_percentage:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a141bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_by_feature = pd.Series(missing_counts, index=feature_cols)\n",
    "top_missing = missing_by_feature.sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 features with most missing values:\")\n",
    "for feature, count in top_missing.items():\n",
    "    print(f\"{feature}: {count} missing values ({count/len(df):.2%})\")\n",
    "\n",
    "# Pre-impute with KNN (same as in training)\n",
    "def preprocess_with_knn(X_missing, k=5):\n",
    "    \"\"\"Preimpute missing values using KNN imputer.\"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    X_imputed = imputer.fit_transform(X_missing)\n",
    "    return X_imputed\n",
    "\n",
    "# Perform KNN imputation on the original data\n",
    "print(\"\\nPerforming initial KNN imputation...\")\n",
    "X_knn_imputed = preprocess_with_knn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_knn_scaled = scaler.fit_transform(X_knn_imputed)\n",
    "\n",
    "# Load the trained I-NAA model\n",
    "print(\"\\nLoading the trained I-NAA model...\")\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = input_dim // 2  # Same as in training\n",
    "model = ImprovedNAA(input_dim, hidden_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load('best_inaa_model.pt'))\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please make sure you've trained the model first.\")\n",
    "    exit()\n",
    "\n",
    "# Use the model to impute missing values\n",
    "print(\"\\nImputing missing values with I-NAA model...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.FloatTensor(X_knn_scaled).to(device)\n",
    "    X_imputed_scaled = model(X_tensor).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0322e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed = scaler.inverse_transform(X_imputed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X.copy()\n",
    "missing_indices = np.where(~mask)\n",
    "X_final[missing_indices] = X_imputed[missing_indices]\n",
    "\n",
    "# Create the fully imputed dataframe\n",
    "df_imputed = df.copy()\n",
    "df_imputed[feature_cols] = X_final\n",
    "\n",
    "# Save the imputed dataframe to a new CSV file\n",
    "output_file = 'physionet_imputed_with_inaa_2.csv'\n",
    "df_imputed.to_csv(output_file, index=False)\n",
    "print(f\"\\nImputed dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imputation_sample(original, imputed, feature_name, n_samples=100):\n",
    "    \"\"\"Plot a sample of original vs imputed values for a specific feature\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get indices of missing values for this feature\n",
    "    feature_idx = feature_cols.index(feature_name)\n",
    "    missing_idx = np.where(~mask[:, feature_idx])[0]\n",
    "    \n",
    "    # If there are missing values for this feature\n",
    "    if len(missing_idx) > 0:\n",
    "        # Limit to n_samples for better visualization\n",
    "        if len(missing_idx) > n_samples:\n",
    "            sample_idx = np.random.choice(missing_idx, n_samples, replace=False)\n",
    "        else:\n",
    "            sample_idx = missing_idx\n",
    "        \n",
    "        # Plot imputed values\n",
    "        plt.scatter(sample_idx, imputed[sample_idx, feature_idx], \n",
    "                   label='Imputed', color='red', alpha=0.7)\n",
    "        \n",
    "        # Plot observed values (non-missing) as reference\n",
    "        observed_idx = np.where(mask[:, feature_idx])[0]\n",
    "        if len(observed_idx) > n_samples:\n",
    "            observed_sample = np.random.choice(observed_idx, n_samples, replace=False)\n",
    "        else:\n",
    "            observed_sample = observed_idx\n",
    "        \n",
    "        plt.scatter(observed_sample, original[observed_sample, feature_idx], \n",
    "                   label='Observed', color='blue', alpha=0.5)\n",
    "        \n",
    "        plt.title(f'Imputation Results for {feature_name}')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'imputation_results_{feature_name}.png')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Imputation visualization for {feature_name} saved as 'imputation_results_{feature_name}.png'\")\n",
    "    else:\n",
    "        print(f\"No missing values for feature {feature_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(top_missing) > 0:\n",
    "    print(\"\\nVisualizing imputation results for top features with missing values...\")\n",
    "    for feature in top_missing.index[:min(3, len(top_missing))]:\n",
    "        plot_imputation_sample(X, X_final, feature)\n",
    "\n",
    "# Print summary statistics before and after imputation\n",
    "print(\"\\nSummary statistics before and after imputation:\")\n",
    "for feature in top_missing.index[:min(5, len(top_missing))]:\n",
    "    # Get original and imputed values for this feature\n",
    "    feature_idx = feature_cols.index(feature)\n",
    "    original_values = X[:, feature_idx]\n",
    "    imputed_values = X_final[:, feature_idx]\n",
    "    \n",
    "    # Calculate statistics for observed values\n",
    "    observed_values = original_values[~np.isnan(original_values)]\n",
    "    \n",
    "    print(f\"\\nFeature: {feature}\")\n",
    "    print(f\"  Before imputation (observed values only):\")\n",
    "    print(f\"    Mean: {np.mean(observed_values):.4f}\")\n",
    "    print(f\"    Std:  {np.std(observed_values):.4f}\")\n",
    "    print(f\"    Min:  {np.min(observed_values):.4f}\")\n",
    "    print(f\"    Max:  {np.max(observed_values):.4f}\")\n",
    "    \n",
    "    print(f\"  After imputation (all values):\")\n",
    "    print(f\"    Mean: {np.mean(imputed_values):.4f}\")\n",
    "    print(f\"    Std:  {np.std(imputed_values):.4f}\")\n",
    "    print(f\"    Min:  {np.min(imputed_values):.4f}\")\n",
    "    print(f\"    Max:  {np.max(imputed_values):.4f}\")\n",
    "\n",
    "print(\"\\nImputation process completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5589c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
