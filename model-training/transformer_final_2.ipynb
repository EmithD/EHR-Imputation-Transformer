{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable relative position encoding for features in the transformer model.\n",
    "    This allows the model to understand relationships between features based on their positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        # Create a learnable embedding for relative positions\n",
    "        self.rel_pos_embedding = nn.Parameter(torch.randn(2 * max_seq_len - 1, d_model))\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply relative positional encodings to the input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with relative positional information.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Create position indices matrix\n",
    "        pos_indices = torch.arange(seq_len, device=x.device)\n",
    "        # Calculate relative positions: for each position i, calculate its relative distance to each position j\n",
    "        rel_pos_indices = pos_indices.unsqueeze(1) - pos_indices.unsqueeze(0) + self.max_seq_len - 1\n",
    "        \n",
    "        # Get embeddings for each relative position\n",
    "        rel_pos_encoded = self.rel_pos_embedding[rel_pos_indices]\n",
    "        \n",
    "        return rel_pos_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithRelPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Relative position encoding\n",
    "        self.rel_pos_encoding = RelativePositionEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Separate linear projection for relative position attention\n",
    "        self.rel_pos_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaling factor for dot product attention\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=False):\n",
    "        \"\"\"\n",
    "        Forward pass with relative positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            query, key, value: Input tensors [batch_size, seq_len, d_model]\n",
    "            key_padding_mask: Mask for padded values [batch_size, seq_len]\n",
    "            need_weights: Whether to return attention weights\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor and optionally attention weights\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        q = self.q_proj(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute content-based attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [batch, heads, seq_len, seq_len]\n",
    "        \n",
    "        # Create relative position bias\n",
    "        # We'll use a simpler approach that's more efficient and avoids shape mismatches\n",
    "        rel_bias = torch.zeros((seq_len, seq_len), device=query.device)\n",
    "        positions = torch.arange(seq_len, device=query.device)\n",
    "        relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)\n",
    "        \n",
    "        # Convert to a simple positional bias (closer = higher attention)\n",
    "        rel_bias = -torch.abs(relative_positions) * 0.1\n",
    "        \n",
    "        # Add the positional bias to the attention scores\n",
    "        # We add the same bias for all heads and batches\n",
    "        attn_scores = attn_scores + rel_bias.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if key_padding_mask is not None:\n",
    "            # Convert mask to attention mask (True = ignore)\n",
    "            attn_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_scores = attn_scores.masked_fill(attn_mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attn_weights, v)  # [batch, heads, seq_len, head_dim]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if need_weights:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, \n",
    "                 activation=\"gelu\", max_seq_len=1000, norm_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention with relative position encoding\n",
    "        self.self_attn = MultiHeadAttentionWithRelPos(\n",
    "            d_model, nhead, dropout=dropout, max_seq_len=max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = getattr(nn.functional, activation)\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len, d_model]\n",
    "            src_key_padding_mask: Mask for padded values [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Processed tensor\n",
    "        \"\"\"\n",
    "        # Pre-norm architecture\n",
    "        if self.norm_first:\n",
    "            # Multi-head attention block with pre-normalization\n",
    "            src2 = self.norm1(src)\n",
    "            src2 = self.self_attn(src2, src2, src2, key_padding_mask=src_key_padding_mask)\n",
    "            src = src + self.dropout1(src2)\n",
    "            \n",
    "            # Feedforward block with pre-normalization\n",
    "            src2 = self.norm2(src)\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "            src = src + self.dropout2(src2)\n",
    "        else:\n",
    "            # Multi-head attention block with post-normalization\n",
    "            src2 = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask)\n",
    "            src = self.norm1(src + self.dropout1(src2))\n",
    "            \n",
    "            # Feedforward block with post-normalization\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "            src = self.norm2(src + self.dropout2(src2))\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len, d_model]\n",
    "            mask: Mask for padded values [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Encoded tensor\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_key_padding_mask=mask)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformerWithRelPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model for tabular data with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 d_model=128, \n",
    "                 nhead=8, \n",
    "                 num_layers=3, \n",
    "                 dim_feedforward=512, \n",
    "                 dropout=0.1, \n",
    "                 activation='gelu',\n",
    "                 max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Feature value embedding\n",
    "        self.value_embedding = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout * 0.5),  # Lower initial dropout\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.feature_interaction = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(d_model, d_model)\n",
    ")\n",
    "        \n",
    "        # Column embedding (learnable)\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        # Layer normalization before transformer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Create encoder layer with relative position encoding\n",
    "        encoder_layer = RelativePositionTransformerLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            max_seq_len=max_seq_len,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        # Create transformer encoder\n",
    "        self.transformer_encoder = RelativePositionTransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output layers with skip connection design\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def _generate_attention_mask(self, mask):\n",
    "        \"\"\"Generate attention mask for transformer\"\"\"\n",
    "        if mask is None:\n",
    "            return None\n",
    "        # Convert binary mask to attention mask (1 = don't attend, 0 = attend)\n",
    "        attn_mask = mask.bool()\n",
    "        return attn_mask\n",
    "                \n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer model with relative position encoding.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, num_features]\n",
    "            column_indices: Tensor of column indices [num_features]\n",
    "            mask: Optional mask for missing values [batch_size, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of predicted values [batch_size, num_features]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Reshape to [batch_size, num_features, 1] for embedding\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Embed feature values\n",
    "        x_embedded = self.value_embedding(x)\n",
    "        \n",
    "        # Add column embeddings\n",
    "        col_embed = self.column_embedding(column_indices).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x_embedded = x_embedded + col_embed\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_embedded = self.norm(x_embedded)\n",
    "        \n",
    "        # Generate attention mask if needed\n",
    "        attn_mask = self._generate_attention_mask(mask) if mask is not None else None\n",
    "        \n",
    "        # Pass through transformer encoder with relative position encoding\n",
    "        x_encoded = self.transformer_encoder(x_embedded, attn_mask)\n",
    "        \n",
    "        # Project to output\n",
    "        output = self.output_projection(x_encoded).squeeze(-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2, mechanism=\"MCAR\"):\n",
    "    \"\"\"\n",
    "    Create a mask for missing values using different mechanisms.\n",
    "    \n",
    "    Args:\n",
    "        data (torch.Tensor): Input data tensor\n",
    "        missing_fraction (float): Fraction of values to mask\n",
    "        mechanism (str): One of \"MCAR\", \"MAR\", or \"MNAR\"\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Binary mask (1 = missing, 0 = present)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # MCAR implementation - completely random\n",
    "        if mechanism == \"MCAR\":\n",
    "            mask = torch.rand(data.shape, device=data.device) < missing_fraction\n",
    "            return mask.int()\n",
    "        \n",
    "        # Simulated MAR (missing at random) implementation\n",
    "        # In MAR, missingness depends on observed values but not on missing values\n",
    "        elif mechanism == \"MAR\":\n",
    "            # Create a base random mask\n",
    "            mask = torch.zeros(data.shape, device=data.device, dtype=torch.int)\n",
    "            \n",
    "            # Number of features\n",
    "            num_features = data.shape[1]\n",
    "            \n",
    "            # For each column, make missingness depend on values in other columns\n",
    "            for col_idx in range(num_features):\n",
    "                # Choose a different column as predictor (wrapping around if needed)\n",
    "                predictor_col = (col_idx + 1) % num_features\n",
    "                \n",
    "                # Get predictor values\n",
    "                predictor_values = data[:, predictor_col]\n",
    "                \n",
    "                # Normalize predictor values to [0, 1] range\n",
    "                if predictor_values.max() > predictor_values.min():\n",
    "                    normalized_values = (predictor_values - predictor_values.min()) / (predictor_values.max() - predictor_values.min())\n",
    "                else:\n",
    "                    normalized_values = torch.zeros_like(predictor_values)\n",
    "                \n",
    "                # Higher predictor values = higher chance of missingness\n",
    "                # Add randomness to avoid making it purely deterministic\n",
    "                prob = normalized_values * 0.5 + torch.rand(data.shape[0], device=data.device) * 0.5\n",
    "                mask[:, col_idx] = (prob > (1 - missing_fraction)).int()\n",
    "            \n",
    "            return mask\n",
    "        \n",
    "        # Simulated MNAR (missing not at random) implementation\n",
    "        # In MNAR, missingness depends on the missing values themselves\n",
    "        elif mechanism == \"MNAR\":\n",
    "            # Create a base random mask\n",
    "            mask = torch.zeros(data.shape, device=data.device, dtype=torch.int)\n",
    "            \n",
    "            # For each column, make missingness depend on its own values\n",
    "            for col_idx in range(data.shape[1]):\n",
    "                # Get column values\n",
    "                col_values = data[:, col_idx]\n",
    "                \n",
    "                # Normalize values to [0, 1] range\n",
    "                if col_values.max() > col_values.min():\n",
    "                    normalized_values = (col_values - col_values.min()) / (col_values.max() - col_values.min())\n",
    "                else:\n",
    "                    normalized_values = torch.zeros_like(col_values)\n",
    "                \n",
    "                # For MNAR, higher values have higher probability of being missing\n",
    "                # Add randomness to avoid making it purely deterministic\n",
    "                prob = normalized_values * 0.7 + torch.rand(data.shape[0], device=data.device) * 0.3\n",
    "                mask[:, col_idx] = (prob > (1 - missing_fraction)).int()\n",
    "            \n",
    "            return mask\n",
    "        \n",
    "        # Default to MCAR if unknown mechanism\n",
    "        else:\n",
    "            print(f\"Unknown missing data mechanism: {mechanism}. Defaulting to MCAR.\")\n",
    "            return create_missing_mask(data, missing_fraction, \"MCAR\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating {mechanism} mask: {e}\")\n",
    "        # Fall back to MCAR if there's an error\n",
    "        return create_missing_mask(data, missing_fraction, \"MCAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_path, test_size=0.2, val_size=0.1, random_state=SEED):\n",
    "    \"\"\"\n",
    "    Load and prepare data for model training.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the CSV file\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        val_size (float): Proportion of training data to use for validation\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Preprocessed train, validation, test tensors, scaler, and column indices\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {data_path}\")\n",
    "    df = pd.read_csv(data_path, index_col=0)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df.isna().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Warning: Dataset contains {missing_count} missing values. These will be handled in preprocessing.\")\n",
    "        # Simple imputation for missing values\n",
    "        df = df.fillna(df.mean())\n",
    "    \n",
    "    # Convert to numpy for preprocessing\n",
    "    data = df.to_numpy()\n",
    "    \n",
    "    # Split data into train and test\n",
    "    train_val_data, test_data = train_test_split(\n",
    "        data, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split train_val into train and validation\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_val_data,\n",
    "        test_size=val_ratio,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    val_data_scaled = scaler.transform(val_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_tensor = torch.tensor(train_data_scaled, dtype=torch.float32).to(device)\n",
    "    val_tensor = torch.tensor(val_data_scaled, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create column indices\n",
    "    column_indices = torch.arange(train_tensor.shape[1]).to(device)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of training samples: {train_tensor.shape[0]}\")\n",
    "    print(f\"Number of validation samples: {val_tensor.shape[0]}\")\n",
    "    print(f\"Number of test samples: {test_tensor.shape[0]}\")\n",
    "    print(f\"Number of features: {train_tensor.shape[1]}\")\n",
    "    \n",
    "    return train_tensor, val_tensor, test_tensor, scaler, column_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask, mechanism=None):\n",
    "    \"\"\"\n",
    "    Compute mean squared error loss on masked positions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values\n",
    "        ground_truth (torch.Tensor): True values\n",
    "        mask (torch.Tensor): Binary mask (1 = missing, 0 = present)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Mean squared error loss\n",
    "    \"\"\"\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(predictions, ground_truth)\n",
    "    \n",
    "    if mechanism == \"MNAR\":\n",
    "        # Higher weight for MNAR samples to focus training\n",
    "        weight = 1.5\n",
    "    else:\n",
    "        weight = 1.0\n",
    "    \n",
    "    # Normalize by number of masked positions\n",
    "    masked_loss = (loss * mask * weight).sum() / ((mask * weight).sum() + 1e-8)\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error on masked positions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values\n",
    "        ground_truth (torch.Tensor): True values\n",
    "        mask (torch.Tensor): Binary mask (1 = missing, 0 = present)\n",
    "        \n",
    "    Returns:\n",
    "        float: RMSE value\n",
    "    \"\"\"\n",
    "    masked_preds = predictions[mask == 1]\n",
    "    masked_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    if len(masked_preds) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mse = torch.mean((masked_preds - masked_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Compute Normalized Root Mean Squared Error on masked positions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values\n",
    "        ground_truth (torch.Tensor): True values\n",
    "        mask (torch.Tensor): Binary mask (1 = missing, 0 = present)\n",
    "        \n",
    "    Returns:\n",
    "        float: NRMSE value\n",
    "    \"\"\"\n",
    "    masked_preds = predictions[mask == 1]\n",
    "    masked_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    if len(masked_preds) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mse = torch.mean((masked_preds - masked_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, missing_fraction, mechanisms=None, scheduler=None):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        dataloader (DataLoader): Training data loader\n",
    "        optimizer (Optimizer): Optimizer\n",
    "        missing_fraction (float): Fraction of values to mask\n",
    "        mechanisms (list): List of missing data mechanisms to use\n",
    "        scheduler (LRScheduler, optional): Learning rate scheduler\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    mixup_prob = 0.3\n",
    "    \n",
    "    if mechanisms is None:\n",
    "        mechanisms = [\"MCAR\"]\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        x = batch[0].to(device)\n",
    "        \n",
    "        # Randomly select a mechanism for this batch\n",
    "        mechanism = random.choice(mechanisms)\n",
    "        \n",
    "        # Create a mask for missing values\n",
    "        mask = create_missing_mask(x, missing_fraction, mechanism)\n",
    "        \n",
    "        # Create input with missing values set to 0\n",
    "        x_masked = x.clone()\n",
    "        x_masked[mask == 1] = 0\n",
    "        \n",
    "        # Apply mixup with probability\n",
    "        if random.random() < mixup_prob:\n",
    "            # Create shuffled indices\n",
    "            indices = torch.randperm(x.size(0), device=device)\n",
    "            \n",
    "            # Mix up samples with lambda drawn from beta distribution\n",
    "            lam = np.random.beta(0.2, 0.2)\n",
    "            \n",
    "            # Mix the data\n",
    "            mixed_x = lam * x_masked + (1 - lam) * x_masked[indices]\n",
    "            mixed_mask = mask | mask[indices]  # Union of masks\n",
    "            \n",
    "            # Forward pass with mixed data\n",
    "            optimizer.zero_grad()\n",
    "            column_indices = torch.arange(x.shape[1], device=device)\n",
    "            predictions = model(mixed_x, column_indices, mixed_mask)\n",
    "            \n",
    "            # Compute mixed loss\n",
    "            loss = lam * compute_loss(predictions, x, mask, mechanism) + \\\n",
    "                   (1 - lam) * compute_loss(predictions, x[indices], mask[indices], mechanism)\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            optimizer.zero_grad()\n",
    "            column_indices = torch.arange(x.shape[1], device=device)\n",
    "            predictions = model(x_masked, column_indices, mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(predictions, x, mask, mechanism)\n",
    "            \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Add gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return {\"loss\": avg_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, missing_fraction, mechanisms=None):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to validate\n",
    "        dataloader (DataLoader): Validation data loader\n",
    "        missing_fraction (float): Fraction of values to mask\n",
    "        mechanisms (list): List of missing data mechanisms to use\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with validation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_rmse = 0\n",
    "    total_nrmse = 0\n",
    "    \n",
    "    if mechanisms is None:\n",
    "        mechanisms = [\"MCAR\"]\n",
    "    \n",
    "    mechanism_metrics = {m: {\"loss\": 0, \"rmse\": 0, \"nrmse\": 0, \"count\": 0} for m in mechanisms}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            for mechanism in mechanisms:\n",
    "                # Create a mask for missing values\n",
    "                mask = create_missing_mask(x, missing_fraction, mechanism)\n",
    "                \n",
    "                # Create input with missing values set to 0\n",
    "                x_masked = x.clone()\n",
    "                x_masked[mask == 1] = 0\n",
    "                \n",
    "                # Forward pass\n",
    "                column_indices = torch.arange(x.shape[1], device=device)\n",
    "                predictions = model(x_masked, column_indices, mask)\n",
    "                \n",
    "                # Compute metrics\n",
    "                loss = compute_loss(predictions, x, mask)\n",
    "                rmse = compute_rmse(predictions, x, mask)\n",
    "                nrmse = compute_nrmse(predictions, x, mask)\n",
    "                \n",
    "                # Update mechanism-specific metrics\n",
    "                mechanism_metrics[mechanism][\"loss\"] += loss.item()\n",
    "                mechanism_metrics[mechanism][\"rmse\"] += rmse\n",
    "                mechanism_metrics[mechanism][\"nrmse\"] += nrmse\n",
    "                mechanism_metrics[mechanism][\"count\"] += 1\n",
    "                \n",
    "                # Update overall metrics\n",
    "                total_loss += loss.item()\n",
    "                total_rmse += rmse\n",
    "                total_nrmse += nrmse\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_evaluations = len(dataloader) * len(mechanisms)\n",
    "    avg_loss = total_loss / num_evaluations\n",
    "    avg_rmse = total_rmse / num_evaluations\n",
    "    avg_nrmse = total_nrmse / num_evaluations\n",
    "    \n",
    "    # Calculate mechanism-specific averages\n",
    "    for m in mechanisms:\n",
    "        if mechanism_metrics[m][\"count\"] > 0:\n",
    "            mechanism_metrics[m][\"loss\"] /= mechanism_metrics[m][\"count\"]\n",
    "            mechanism_metrics[m][\"rmse\"] /= mechanism_metrics[m][\"count\"]\n",
    "            mechanism_metrics[m][\"nrmse\"] /= mechanism_metrics[m][\"count\"]\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rmse\": avg_rmse,\n",
    "        \"nrmse\": avg_nrmse,\n",
    "        \"mechanisms\": mechanism_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, column_indices, config):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Args:\n",
    "        train_data (torch.Tensor): Training data\n",
    "        val_data (torch.Tensor): Validation data\n",
    "        column_indices (torch.Tensor): Column indices\n",
    "        config (dict): Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Trained model and training history\n",
    "    \"\"\"\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(train_data)\n",
    "    val_dataset = TensorDataset(val_data)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    num_features = train_data.shape[1]\n",
    "    model = TabularTransformerWithRelPos(\n",
    "        num_features=num_features,\n",
    "        d_model=config[\"d_model\"],\n",
    "        nhead=config[\"num_heads\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dim_feedforward=config[\"dim_feedforward\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        activation=config[\"activation\"],\n",
    "        max_seq_len=max(2 * num_features, 100)  # Set max_seq_len based on feature count\n",
    "    ).to(device)\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler - use OneCycleLR for better convergence\n",
    "    total_steps = len(train_dataloader) * config[\"num_epochs\"]\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config[\"learning_rate\"],\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.3,  # Warm-up phase percentage\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,  # Initial lr = max_lr/div_factor\n",
    "        final_div_factor=10000.0  # Final lr = initial_lr/final_div_factor\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_rmse\": [],\n",
    "        \"val_nrmse\": [],\n",
    "        \"val_mcar_nrmse\": [],\n",
    "        \"val_mar_nrmse\": [],\n",
    "        \"val_mnar_nrmse\": [],\n",
    "        \"lr\": []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            config[\"missing_fraction\"],\n",
    "            mechanisms,\n",
    "            scheduler\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            config[\"missing_fraction\"],\n",
    "            mechanisms\n",
    "        )\n",
    "        \n",
    "        # Update history\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_rmse\"].append(val_metrics[\"rmse\"])\n",
    "        history[\"val_nrmse\"].append(val_metrics[\"nrmse\"])\n",
    "        history[\"val_mcar_nrmse\"].append(val_metrics[\"mechanisms\"][\"MCAR\"][\"nrmse\"])\n",
    "        history[\"val_mar_nrmse\"].append(val_metrics[\"mechanisms\"][\"MAR\"][\"nrmse\"])\n",
    "        history[\"val_mnar_nrmse\"].append(val_metrics[\"mechanisms\"][\"MNAR\"][\"nrmse\"])\n",
    "        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Val NRMSE: {val_metrics['nrmse']:.4f}\")\n",
    "        print(f\"Val MCAR NRMSE: {val_metrics['mechanisms']['MCAR']['nrmse']:.4f}\")\n",
    "        print(f\"Val MAR NRMSE: {val_metrics['mechanisms']['MAR']['nrmse']:.4f}\")\n",
    "        print(f\"Val MNAR NRMSE: {val_metrics['mechanisms']['MNAR']['nrmse']:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_metrics[\"loss\"] < best_val_loss:\n",
    "            best_val_loss = val_metrics[\"loss\"]\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config[\"patience\"]:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training history.\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Training history\n",
    "        save_path (str, optional): Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot NRMSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history[\"val_nrmse\"], label=\"Overall NRMSE\")\n",
    "    plt.plot(history[\"val_mcar_nrmse\"], label=\"MCAR NRMSE\")\n",
    "    plt.plot(history[\"val_mar_nrmse\"], label=\"MAR NRMSE\")\n",
    "    plt.plot(history[\"val_mnar_nrmse\"], label=\"MNAR NRMSE\")\n",
    "    plt.title(\"NRMSE by Mechanism\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"NRMSE\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot learning rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history[\"lr\"])\n",
    "    plt.title(\"Learning Rate\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, config, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Save model and related objects.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        scaler (StandardScaler): Data scaler\n",
    "        config (dict): Model configuration\n",
    "        save_dir (str): Directory to save model\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(save_dir, \"tabular_transformer_relpos.pth\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, model_path)\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = os.path.join(save_dir, \"scaler.pkl\")\n",
    "    import pickle\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Scaler saved to {scaler_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate_model(data, column_indices, config, n_folds=5):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{n_folds}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_data = data[train_idx]\n",
    "        val_data = data[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model, history = train_model(train_data, val_data, column_indices, config)\n",
    "        \n",
    "        # Validate\n",
    "        val_dataloader = DataLoader(TensorDataset(val_data), batch_size=config[\"batch_size\"])\n",
    "        metrics = validate(model, val_dataloader, config[\"missing_fraction\"], [\"MCAR\", \"MAR\", \"MNAR\"])\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"Fold {fold+1} metrics:\")\n",
    "        print(f\"  NRMSE: {metrics['nrmse']:.4f}\")\n",
    "        print(f\"  MCAR NRMSE: {metrics['mechanisms']['MCAR']['nrmse']:.4f}\")\n",
    "        print(f\"  MAR NRMSE: {metrics['mechanisms']['MAR']['nrmse']:.4f}\")\n",
    "        print(f\"  MNAR NRMSE: {metrics['mechanisms']['MNAR']['nrmse']:.4f}\")\n",
    "    \n",
    "    # Average metrics across folds\n",
    "    avg_metrics = {\n",
    "        \"nrmse\": np.mean([m[\"nrmse\"] for m in all_metrics]),\n",
    "        \"mcar_nrmse\": np.mean([m[\"mechanisms\"][\"MCAR\"][\"nrmse\"] for m in all_metrics]),\n",
    "        \"mar_nrmse\": np.mean([m[\"mechanisms\"][\"MAR\"][\"nrmse\"] for m in all_metrics]),\n",
    "        \"mnar_nrmse\": np.mean([m[\"mechanisms\"][\"MNAR\"][\"nrmse\"] for m in all_metrics])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAverage metrics across folds:\")\n",
    "    print(f\"  NRMSE: {avg_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MCAR NRMSE: {avg_metrics['mcar_nrmse']:.4f}\")\n",
    "    print(f\"  MAR NRMSE: {avg_metrics['mar_nrmse']:.4f}\")\n",
    "    print(f\"  MNAR NRMSE: {avg_metrics['mnar_nrmse']:.4f}\")\n",
    "    \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution code\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        # Data parameters\n",
    "        \"data_path\": \"./data/physionet_39_features_only.csv\",\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.1,\n",
    "        \n",
    "        # Model parameters\n",
    "        \"d_model\": 384,              # Increased from 256\n",
    "        \"num_heads\": 12,             # Increased from 8\n",
    "        \"num_layers\": 6,             # Increased from 5\n",
    "        \"dim_feedforward\": 1536,     # Increased from 1024\n",
    "        \"dropout\": 0.15,             # Reduced from 0.2 for better generalization\n",
    "        \"activation\": \"gelu\",\n",
    "        \n",
    "        # Training parameters\n",
    "        \"batch_size\": 128,           # Increased from 64\n",
    "        \"learning_rate\": 0.0015,     # Slightly higher for faster initial training\n",
    "        \"weight_decay\": 0.005,       # Reduced from 0.01\n",
    "        \"num_epochs\": 200,           # Doubled from 100\n",
    "        \"patience\": 25,              # Increased from 15\n",
    "        \"missing_fraction\": 0.3,\n",
    "        \n",
    "        \"warmup_steps_pct\": 0.2,     # Shorter warmup phase\n",
    "        \"min_lr_factor\": 5000.0,    # Slower final decay\n",
    "        \n",
    "        # Save parameters\n",
    "        \"save_dir\": \"models\"\n",
    "    }\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = os.path.join(config[\"save_dir\"], f\"{current_time}_model\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    train_data, val_data, test_data, scaler, column_indices = load_and_prepare_data(\n",
    "        config[\"data_path\"],\n",
    "        config[\"test_size\"],\n",
    "        config[\"val_size\"]\n",
    "    )\n",
    "    \n",
    "    # Add flag to control whether to run k-fold validation\n",
    "    RUN_KFOLD = True\n",
    "    \n",
    "    if RUN_KFOLD:\n",
    "        print(\"\\n=== Running K-Fold Cross-Validation ===\")\n",
    "        # Combine train and validation data for k-fold\n",
    "        combined_data = torch.cat([train_data, val_data], dim=0)\n",
    "        \n",
    "        # Set a smaller number of epochs for k-fold to save time\n",
    "        kfold_config = config.copy()\n",
    "        kfold_config[\"num_epochs\"] = 100  # Reduced epochs for k-fold\n",
    "        \n",
    "        # Run k-fold validation\n",
    "        cv_metrics = cross_validate_model(\n",
    "            combined_data, \n",
    "            column_indices, \n",
    "            kfold_config, \n",
    "            n_folds=5\n",
    "        )\n",
    "        \n",
    "        # Save cross-validation results\n",
    "        cv_results_path = os.path.join(save_dir, \"cv_results.txt\")\n",
    "        with open(cv_results_path, \"w\") as f:\n",
    "            f.write(\"Cross-Validation Results:\\n\")\n",
    "            f.write(f\"  NRMSE: {cv_metrics['nrmse']:.4f}\\n\")\n",
    "            f.write(f\"  MCAR NRMSE: {cv_metrics['mcar_nrmse']:.4f}\\n\")\n",
    "            f.write(f\"  MAR NRMSE: {cv_metrics['mar_nrmse']:.4f}\\n\")\n",
    "            f.write(f\"  MNAR NRMSE: {cv_metrics['mnar_nrmse']:.4f}\\n\")\n",
    "    \n",
    "    # Train final model on the entire training set\n",
    "    print(\"\\n=== Training Final Model ===\")\n",
    "    model, history = train_model(train_data, val_data, column_indices, config)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, os.path.join(save_dir, \"training_history.png\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n=== Evaluating on Test Set ===\")\n",
    "    test_dataloader = DataLoader(TensorDataset(test_data), batch_size=config[\"batch_size\"])\n",
    "    test_metrics = validate(model, test_dataloader, config[\"missing_fraction\"], [\"MCAR\", \"MAR\", \"MNAR\"])\n",
    "    \n",
    "    print(f\"Test metrics:\")\n",
    "    print(f\"  Overall NRMSE: {test_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MCAR NRMSE: {test_metrics['mechanisms']['MCAR']['nrmse']:.4f}\")\n",
    "    print(f\"  MAR NRMSE: {test_metrics['mechanisms']['MAR']['nrmse']:.4f}\")\n",
    "    print(f\"  MNAR NRMSE: {test_metrics['mechanisms']['MNAR']['nrmse']:.4f}\")\n",
    "    \n",
    "    # Save test results\n",
    "    test_results_path = os.path.join(save_dir, \"test_results.txt\")\n",
    "    with open(test_results_path, \"w\") as f:\n",
    "        f.write(\"Test Results:\\n\")\n",
    "        f.write(f\"  Overall NRMSE: {test_metrics['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MCAR NRMSE: {test_metrics['mechanisms']['MCAR']['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MAR NRMSE: {test_metrics['mechanisms']['MAR']['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MNAR NRMSE: {test_metrics['mechanisms']['MNAR']['nrmse']:.4f}\\n\")\n",
    "    \n",
    "    # Save model\n",
    "    save_model(model, scaler, config, save_dir)\n",
    "\n",
    "    print(f\"Training complete! Model saved in: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
