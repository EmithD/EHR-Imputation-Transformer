{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import math\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCorrelationModule(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that explicitly models feature correlations to better handle MNAR scenarios.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.correlation_proj = nn.Linear(d_model, d_model)\n",
    "        self.feature_gate = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.correlation_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features [batch_size, num_features, d_model]\n",
    "            mask: Missing value mask [batch_size, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Correlation-enhanced features\n",
    "        \"\"\"\n",
    "        batch_size, num_features, d_model = x.size()\n",
    "        \n",
    "        # Project features for correlation computation\n",
    "        x_proj = self.correlation_proj(x)\n",
    "        \n",
    "        # Compute pairwise feature correlations (scaled dot-product)\n",
    "        corr_matrix = torch.bmm(x_proj, x_proj.transpose(1, 2)) / math.sqrt(d_model)\n",
    "        \n",
    "        # If we have a mask, adjust correlation for missing values\n",
    "        if mask is not None:\n",
    "            # Create attention mask (1 for observed values, 0 for missing)\n",
    "            obs_mask = 1 - mask.float()\n",
    "            mask_matrix = torch.bmm(obs_mask.unsqueeze(2), obs_mask.unsqueeze(1))\n",
    "            \n",
    "            # Apply mask to correlation matrix (masked positions get -1e9)\n",
    "            masked_corr = corr_matrix.masked_fill(mask_matrix == 0, -1e9)\n",
    "            \n",
    "            # Softmax to get normalized correlation weights\n",
    "            corr_weights = F.softmax(masked_corr, dim=-1)\n",
    "        else:\n",
    "            corr_weights = F.softmax(corr_matrix, dim=-1)\n",
    "        \n",
    "        # Apply correlation weights to propagate information across features\n",
    "        corr_features = torch.bmm(corr_weights, x)\n",
    "        \n",
    "        # Compute feature-specific gates to control information flow\n",
    "        gates = self.feature_gate(x)\n",
    "        \n",
    "        # Gate the correlation features and apply residual connection\n",
    "        gated_corr = gates * corr_features\n",
    "        enhanced_features = x + self.dropout(gated_corr)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        enhanced_features = self.correlation_norm(enhanced_features)\n",
    "        \n",
    "        return enhanced_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureValueDependentEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An enhanced module that explicitly models the relationship between feature values and missingness,\n",
    "    which is crucial for MNAR scenarios.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Deep value encoder with skip connections\n",
    "        self.value_encoder_1 = nn.Linear(d_model, d_model * 2)\n",
    "        self.value_encoder_2 = nn.Linear(d_model * 2, d_model * 2)\n",
    "        self.value_encoder_3 = nn.Linear(d_model * 2, d_model)\n",
    "        \n",
    "        self.value_norm_1 = nn.LayerNorm(d_model * 2)\n",
    "        self.value_norm_2 = nn.LayerNorm(d_model * 2)\n",
    "        self.value_norm_3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Enhanced missingness encoder with more capacity\n",
    "        self.missingness_encoder_1 = nn.Linear(1, d_model)\n",
    "        self.missingness_encoder_2 = nn.Linear(d_model, d_model)\n",
    "        self.missingness_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Multi-head attention for better fusion\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads=8, dropout=dropout)\n",
    "        \n",
    "        # Gated cross-attention for value-missingness interaction\n",
    "        self.cross_gate = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Enhanced fusion layer\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 1.5),  # Increased dropout\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features [batch_size, num_features, d_model]\n",
    "            mask: Missing value mask [batch_size, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Features with enhanced MNAR understanding\n",
    "        \"\"\"\n",
    "        # Encode feature values with residual connections\n",
    "        value_enc_1 = self.value_norm_1(F.gelu(self.value_encoder_1(x)))\n",
    "        value_enc_2 = self.value_norm_2(F.gelu(self.value_encoder_2(value_enc_1)) + value_enc_1)\n",
    "        value_encoding = self.value_norm_3(F.gelu(self.value_encoder_3(value_enc_2)))\n",
    "        \n",
    "        # If we have a mask, encode missingness patterns\n",
    "        if mask is not None:\n",
    "            # Encode missingness with non-linear transformations\n",
    "            mask_enc_1 = F.gelu(self.missingness_encoder_1(mask.float().unsqueeze(-1)))\n",
    "            mask_encoding = self.missingness_norm(F.gelu(self.missingness_encoder_2(mask_enc_1)))\n",
    "            \n",
    "            # Reshape for attention: [seq_len, batch_size, d_model]\n",
    "            batch_size, num_features, d_model = value_encoding.size()\n",
    "            v_enc = value_encoding.transpose(0, 1)\n",
    "            m_enc = mask_encoding.transpose(0, 1)\n",
    "            \n",
    "            # Self-attention on value encodings with mask as query\n",
    "            attn_output, attn_weights = self.attention(m_enc, v_enc, v_enc)\n",
    "            \n",
    "            # Reshape back: [batch_size, seq_len, d_model]\n",
    "            attn_output = attn_output.transpose(0, 1)\n",
    "            \n",
    "            # Feature-specific gating based on value and missingness\n",
    "            combined_for_gate = torch.cat([mask_encoding, attn_output], dim=-1)\n",
    "            gate = self.cross_gate(combined_for_gate)\n",
    "            \n",
    "            # Gated combination\n",
    "            gated_attn = gate * attn_output\n",
    "            \n",
    "            # Enhanced mask encoding with attention output\n",
    "            mask_encoding = mask_encoding + self.dropout(gated_attn)\n",
    "            \n",
    "            # Concatenate value and enhanced missingness encodings\n",
    "            combined = torch.cat([value_encoding, mask_encoding], dim=-1)\n",
    "            enhanced = self.fusion_layer(combined)\n",
    "            \n",
    "            # Add residual connection\n",
    "            enhanced = enhanced + value_encoding\n",
    "        else:\n",
    "            # Without mask, just use value encoding\n",
    "            enhanced = value_encoding\n",
    "            \n",
    "        return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable relative position encoding for features in the transformer model.\n",
    "    This allows the model to understand relationships between features based on their positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.rel_pos_embedding = nn.Parameter(torch.randn(2 * max_seq_len - 1, d_model))\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply relative positional encodings to the input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with relative positional information.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        pos_indices = torch.arange(seq_len, device=x.device)\n",
    "        rel_pos_indices = pos_indices.unsqueeze(1) - pos_indices.unsqueeze(0) + self.max_seq_len - 1\n",
    "        rel_pos_encoded = self.rel_pos_embedding[rel_pos_indices]\n",
    "        \n",
    "        return rel_pos_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithRelPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.rel_pos_encoding = RelativePositionEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.rel_pos_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, key_padding_mask=None, need_weights=False):\n",
    "        \"\"\"\n",
    "        Forward pass with relative positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            query, key, value: Input tensors [batch_size, seq_len, d_model]\n",
    "            key_padding_mask: Mask for padded values [batch_size, seq_len]\n",
    "            need_weights: Whether to return attention weights\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor and optionally attention weights\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        #Linear projections and reshape for multi-head attention\n",
    "        q = self.q_proj(query).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        #content-based attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [batch, heads, seq_len, seq_len]\n",
    "        \n",
    "        rel_bias = torch.zeros((seq_len, seq_len), device=query.device)\n",
    "        positions = torch.arange(seq_len, device=query.device)\n",
    "        relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)\n",
    "\n",
    "        rel_bias = -torch.abs(relative_positions) * 0.1\n",
    "\n",
    "        attn_scores = attn_scores + rel_bias.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attn_scores = attn_scores.masked_fill(attn_mask, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        if need_weights:\n",
    "            return output, attn_weights\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, \n",
    "                 activation=\"gelu\", max_seq_len=1000, norm_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttentionWithRelPos(\n",
    "            d_model, nhead, dropout=dropout, max_seq_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        #FFN\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        #Activation\n",
    "        self.activation = getattr(nn.functional, activation)\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len, d_model]\n",
    "            src_key_padding_mask: Mask for padded values [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Processed tensor\n",
    "        \"\"\"\n",
    "        # Pre-norm architecture\n",
    "        if self.norm_first:\n",
    "            src2 = self.norm1(src)\n",
    "            src2 = self.self_attn(src2, src2, src2, key_padding_mask=src_key_padding_mask)\n",
    "            src = src + self.dropout1(src2)\n",
    "            \n",
    "            src2 = self.norm2(src)\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "            src = src + self.dropout2(src2)\n",
    "        else:\n",
    "            # Multi-head attention block with post-normalization\n",
    "            src2 = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask)\n",
    "            src = self.norm1(src + self.dropout1(src2))\n",
    "            \n",
    "            # Feedforward block with post-normalization\n",
    "            src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "            src = self.norm2(src + self.dropout2(src2))\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder with relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, src, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len, d_model]\n",
    "            mask: Mask for padded values [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Encoded tensor\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_key_padding_mask=mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformerWithRelPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced transformer model for tabular data imputation with improved\n",
    "    MNAR handling through correlation modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_features, \n",
    "                 d_model=128, \n",
    "                 nhead=8, \n",
    "                 num_layers=3, \n",
    "                 dim_feedforward=512, \n",
    "                 dropout=0.1, \n",
    "                 activation='gelu',\n",
    "                 max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Feature value embedding\n",
    "        self.value_embedding = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Column embedding (learnable)\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        # Missing value embedding\n",
    "        self.missing_embedding = nn.Parameter(torch.randn(1, d_model))\n",
    "        \n",
    "        # Feature correlation module to model dependencies between features\n",
    "        self.feature_correlation = FeatureCorrelationModule(num_features, d_model, dropout)\n",
    "        \n",
    "        # Feature-value dependent encoder for MNAR awareness\n",
    "        self.feature_value_encoder = FeatureValueDependentEncoder(d_model, dropout)\n",
    "        \n",
    "        # Layer normalization before transformer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Create transformer encoder layers\n",
    "        encoder_layer = RelativePositionTransformerLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            max_seq_len=max_seq_len,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        # Create transformer encoder\n",
    "        self.transformer_encoder = RelativePositionTransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                \n",
    "    def _generate_attention_mask(self, mask):\n",
    "        \"\"\"Generate attention mask for transformer\"\"\"\n",
    "        if mask is None:\n",
    "            return None\n",
    "        attn_mask = mask.bool()\n",
    "        return attn_mask\n",
    "                \n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass with enhanced correlation modeling for MNAR patterns.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, num_features]\n",
    "            column_indices: Tensor of column indices [num_features]\n",
    "            mask: Optional mask for missing values [batch_size, num_features]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of predicted values [batch_size, num_features]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Reshape to [batch_size, num_features, 1] for embedding\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Embed feature values\n",
    "        x_embedded = self.value_embedding(x)\n",
    "        \n",
    "        # Add column embeddings\n",
    "        col_embed = self.column_embedding(column_indices).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x_embedded = x_embedded + col_embed\n",
    "        \n",
    "        # Apply special handling for missing values\n",
    "        if mask is not None:\n",
    "            # Expand mask for broadcasting\n",
    "            mask_expanded = mask.unsqueeze(-1).expand_as(x_embedded)\n",
    "            # Replace embeddings for missing values with learned missing embedding\n",
    "            missing_embed = self.missing_embedding.expand_as(x_embedded)\n",
    "            x_embedded = torch.where(mask_expanded == 1, missing_embed, x_embedded)\n",
    "        \n",
    "        # Enhanced feature correlation modeling - key for MNAR patterns\n",
    "        x_correlated = self.feature_correlation(x_embedded, mask)\n",
    "        \n",
    "        # Feature-value dependent encoding - explicitly models relationship \n",
    "        # between values and missingness\n",
    "        x_value_aware = self.feature_value_encoder(x_correlated, mask)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_embedded = self.norm(x_value_aware)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        attn_mask = self._generate_attention_mask(mask) if mask is not None else None\n",
    "        x_encoded = self.transformer_encoder(x_embedded, attn_mask)\n",
    "        \n",
    "        # Project to output\n",
    "        output = self.output_projection(x_encoded).squeeze(-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Ensemble of transformer models for improved prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, config, num_models=3):\n",
    "        super().__init__()\n",
    "        self.num_models = num_models\n",
    "        \n",
    "        # Create multiple base models\n",
    "        self.models = nn.ModuleList([\n",
    "            TabularTransformerWithRelPos(\n",
    "                num_features=num_features,\n",
    "                d_model=config[\"d_model\"],\n",
    "                nhead=config[\"num_heads\"],\n",
    "                num_layers=config[\"num_layers\"],\n",
    "                dim_feedforward=config[\"dim_feedforward\"],\n",
    "                dropout=config[\"dropout\"],\n",
    "                activation=config[\"activation\"],\n",
    "                max_seq_len=max(2 * num_features, 100)\n",
    "            ) for _ in range(num_models)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "        # Get predictions from all models\n",
    "        all_preds = []\n",
    "        for model in self.models:\n",
    "            preds = model(x, column_indices, mask)\n",
    "            all_preds.append(preds.unsqueeze(0))\n",
    "        \n",
    "        # Stack and average predictions\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        avg_preds = torch.mean(all_preds, dim=0)\n",
    "        \n",
    "        return avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2, mechanism=\"MCAR\"):\n",
    "    \"\"\"\n",
    "    Create a mask for missing values using different mechanisms, with enhanced MAR modeling.\n",
    "    \n",
    "    Args:\n",
    "        data (torch.Tensor): Input data tensor\n",
    "        missing_fraction (float): Fraction of values to mask\n",
    "        mechanism (str): One of \"MCAR\", \"MAR\", or \"MNAR\"\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Binary mask (1 = missing, 0 = present)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # MCAR implementation - completely random\n",
    "        if mechanism == \"MCAR\":\n",
    "            mask = torch.rand(data.shape, device=data.device) < missing_fraction\n",
    "            return mask.int()\n",
    "        \n",
    "        # Simulated MAR (missing at random) implementation with enhanced realism\n",
    "        # In MAR, missingness depends on observed values but not on missing values\n",
    "        elif mechanism == \"MAR\":\n",
    "            # Create empty mask\n",
    "            mask = torch.zeros(data.shape, device=data.device, dtype=torch.int)\n",
    "            \n",
    "            # Number of features\n",
    "            num_features = data.shape[1]\n",
    "            batch_size = data.shape[0]\n",
    "            \n",
    "            # Feature dependencies - which features influence the missingness of others\n",
    "            # In real data, missing patterns often form groups or clusters\n",
    "            \n",
    "            # Create feature groups (simulation of related features)\n",
    "            num_groups = min(5, num_features // 2)\n",
    "            features_per_group = num_features // num_groups\n",
    "            \n",
    "            for group_idx in range(num_groups):\n",
    "                # Get features in this group\n",
    "                start_idx = group_idx * features_per_group\n",
    "                end_idx = min(start_idx + features_per_group, num_features)\n",
    "                group_features = list(range(start_idx, end_idx))\n",
    "                \n",
    "                if len(group_features) < 2:\n",
    "                    continue  # Skip too small groups\n",
    "                \n",
    "                # For each feature in the group, select 1-3 other features from the group as predictors\n",
    "                for feature_idx in group_features:\n",
    "                    # Choose predictor features from the same group but excluding the current feature\n",
    "                    other_features = [f for f in group_features if f != feature_idx]\n",
    "                    \n",
    "                    # Determine how many predictors to use based on group size\n",
    "                    num_predictors = min(3, len(other_features))\n",
    "                    \n",
    "                    if num_predictors == 0:\n",
    "                        # Fallback to MCAR for this feature if no predictors available\n",
    "                        mask[:, feature_idx] = (torch.rand(batch_size, device=data.device) < missing_fraction).int()\n",
    "                        continue\n",
    "                    \n",
    "                    # Randomly select predictor features\n",
    "                    predictor_indices = torch.tensor(other_features, device=data.device)[\n",
    "                        torch.randperm(len(other_features), device=data.device)[:num_predictors]\n",
    "                    ]\n",
    "                    \n",
    "                    # Compute the prediction function - more complex than simple sum\n",
    "                    # Use a combination of sum, product, and max to create more realistic dependencies\n",
    "                    pred_values = torch.zeros(batch_size, device=data.device)\n",
    "                    \n",
    "                    for i, p_idx in enumerate(predictor_indices):\n",
    "                        if i == 0:\n",
    "                            # First predictor is base\n",
    "                            pred_values = data[:, p_idx]\n",
    "                        elif i == 1:\n",
    "                            # Second predictor creates interaction effect\n",
    "                            pred_values = pred_values * (1 + 0.5 * data[:, p_idx])\n",
    "                        else:\n",
    "                            # Third+ predictor has conditional effect\n",
    "                            pred_values = pred_values + 0.3 * torch.where(\n",
    "                                data[:, p_idx] > 0, \n",
    "                                data[:, p_idx], \n",
    "                                torch.zeros_like(data[:, p_idx])\n",
    "                            )\n",
    "                    \n",
    "                    # Normalize to [0, 1] range\n",
    "                    if pred_values.max() > pred_values.min():\n",
    "                        normalized_values = (pred_values - pred_values.min()) / (pred_values.max() - pred_values.min())\n",
    "                    else:\n",
    "                        normalized_values = torch.zeros_like(pred_values)\n",
    "                    \n",
    "                    # Apply non-linear transformation with asymmetric threshold\n",
    "                    # Create a more realistic pattern with step function and noise\n",
    "                    \n",
    "                    # Step function with noise\n",
    "                    noise = torch.rand(batch_size, device=data.device) * 0.3  # Random noise\n",
    "                    \n",
    "                    # Apply threshold based on normalized values and missing fraction\n",
    "                    # Calculate adaptive threshold to maintain overall missing rate\n",
    "                    sorted_values, _ = torch.sort(normalized_values + noise)\n",
    "                    threshold_idx = int((1 - missing_fraction) * batch_size)\n",
    "                    if threshold_idx < batch_size:\n",
    "                        threshold = sorted_values[threshold_idx]\n",
    "                    else:\n",
    "                        threshold = 1.1  # Ensures no missing values if threshold_idx is out of range\n",
    "                    \n",
    "                    # Create binary mask\n",
    "                    mask[:, feature_idx] = (normalized_values + noise > threshold).int()\n",
    "            \n",
    "            # Fill any remaining features with MCAR\n",
    "            for feature_idx in range(num_features):\n",
    "                if mask[:, feature_idx].sum() == 0:  # No missing values assigned yet\n",
    "                    mask[:, feature_idx] = (torch.rand(batch_size, device=data.device) < missing_fraction).int()\n",
    "            \n",
    "            # Ensure we have the correct overall missing rate\n",
    "            actual_missing_rate = mask.float().mean().item()\n",
    "            \n",
    "            # Adjust if needed\n",
    "            if abs(actual_missing_rate - missing_fraction) > 0.05:\n",
    "                # Too much difference, adjust by adding/removing some random missing values\n",
    "                if actual_missing_rate < missing_fraction:\n",
    "                    # Need to add more missing values\n",
    "                    additional_mask = torch.rand(data.shape, device=data.device) < (missing_fraction - actual_missing_rate) * 2\n",
    "                    additional_mask = additional_mask.int() & (1 - mask)  # Only add where not already missing\n",
    "                    mask = mask | additional_mask\n",
    "                else:\n",
    "                    # Need to remove some missing values\n",
    "                    removal_mask = torch.rand(data.shape, device=data.device) < (actual_missing_rate - missing_fraction) * 2\n",
    "                    removal_mask = removal_mask.int() & mask  # Only remove from missing values\n",
    "                    mask = mask & (1 - removal_mask)\n",
    "            \n",
    "            return mask\n",
    "        \n",
    "        # Simulated MNAR (missing not at random) implementation\n",
    "        # In MNAR, missingness depends on the missing values themselves\n",
    "        elif mechanism == \"MNAR\":\n",
    "            # Create a base random mask\n",
    "            mask = torch.zeros(data.shape, device=data.device, dtype=torch.int)\n",
    "            \n",
    "            # Number of features\n",
    "            num_features = data.shape[1]\n",
    "            batch_size = data.shape[0]\n",
    "            \n",
    "            # Group features for more realistic patterns\n",
    "            # Some features may have MNAR patterns based on their own values\n",
    "            # Others may have MNAR patterns based on related features\n",
    "            \n",
    "            # Determine pattern type for each feature (0: own value, 1: extreme values, 2: related feature)\n",
    "            pattern_types = torch.randint(0, 3, (num_features,), device=data.device)\n",
    "            \n",
    "            for col_idx in range(num_features):\n",
    "                pattern_type = pattern_types[col_idx].item()\n",
    "                \n",
    "                if pattern_type == 0:\n",
    "                    # Pattern based on own value - higher values more likely to be missing\n",
    "                    # This is a common MNAR pattern in medical data\n",
    "                    col_values = data[:, col_idx]\n",
    "                    \n",
    "                    # Normalize to [0, 1]\n",
    "                    if col_values.max() > col_values.min():\n",
    "                        normalized_values = (col_values - col_values.min()) / (col_values.max() - col_values.min())\n",
    "                    else:\n",
    "                        normalized_values = torch.zeros_like(col_values)\n",
    "                    \n",
    "                    # Higher values have higher probability of being missing\n",
    "                    # Add noise for more realistic pattern\n",
    "                    prob = normalized_values * 0.8 + torch.rand(batch_size, device=data.device) * 0.2\n",
    "                    \n",
    "                elif pattern_type == 1:\n",
    "                    # Pattern based on extreme values (both high and low)\n",
    "                    # This is common in censored data (values outside measurement range)\n",
    "                    col_values = data[:, col_idx]\n",
    "                    \n",
    "                    # Normalize to [-1, 1] centered at 0\n",
    "                    if col_values.max() > col_values.min():\n",
    "                        normalized_values = 2 * (col_values - col_values.min()) / (col_values.max() - col_values.min()) - 1\n",
    "                    else:\n",
    "                        normalized_values = torch.zeros_like(col_values)\n",
    "                    \n",
    "                    # Extreme values (away from center) more likely to be missing\n",
    "                    extremeness = torch.abs(normalized_values)\n",
    "                    prob = extremeness * 0.7 + torch.rand(batch_size, device=data.device) * 0.3\n",
    "                    \n",
    "                else:\n",
    "                    # Pattern based on related feature (still MNAR but more complex)\n",
    "                    # Choose random other feature\n",
    "                    other_idx = (col_idx + torch.randint(1, num_features, (1,), device=data.device).item()) % num_features\n",
    "                    \n",
    "                    # Use combination of own value and other feature\n",
    "                    own_values = data[:, col_idx]\n",
    "                    other_values = data[:, other_idx]\n",
    "                    \n",
    "                    # Normalize both\n",
    "                    if own_values.max() > own_values.min():\n",
    "                        norm_own = (own_values - own_values.min()) / (own_values.max() - own_values.min())\n",
    "                    else:\n",
    "                        norm_own = torch.zeros_like(own_values)\n",
    "                        \n",
    "                    if other_values.max() > other_values.min():\n",
    "                        norm_other = (other_values - other_values.min()) / (other_values.max() - other_values.min())\n",
    "                    else:\n",
    "                        norm_other = torch.zeros_like(other_values)\n",
    "                    \n",
    "                    # Complex interaction: missing when both values are high or both are low\n",
    "                    interaction = 1 - 2 * torch.abs(norm_own - norm_other)  # High when similar, low when different\n",
    "                    prob = interaction * 0.7 + torch.rand(batch_size, device=data.device) * 0.3\n",
    "                \n",
    "                # Apply threshold to maintain missing rate\n",
    "                sorted_probs, _ = torch.sort(prob)\n",
    "                threshold_idx = int((1 - missing_fraction) * batch_size)\n",
    "                if threshold_idx < batch_size:\n",
    "                    threshold = sorted_probs[threshold_idx]\n",
    "                else:\n",
    "                    threshold = 1.1  # Ensures no missing values if threshold_idx is out of range\n",
    "                \n",
    "                # Set mask\n",
    "                mask[:, col_idx] = (prob > threshold).int()\n",
    "            \n",
    "            return mask\n",
    "        \n",
    "        # Default to MCAR if unknown mechanism\n",
    "        else:\n",
    "            print(f\"Unknown missing data mechanism: {mechanism}. Defaulting to MCAR.\")\n",
    "            return create_missing_mask(data, missing_fraction, \"MCAR\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating {mechanism} mask: {e}\")\n",
    "        # Fall back to MCAR if there's an error\n",
    "        return create_missing_mask(data, missing_fraction, \"MCAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_path, test_size=0.2, val_size=0.1, random_state=SEED):\n",
    "    \"\"\"\n",
    "    Load and prepare data for model training.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the CSV file\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        val_size (float): Proportion of training data to use for validation\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Preprocessed train, validation, test tensors, scaler, and column indices\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {data_path}\")\n",
    "    df = pd.read_csv(data_path, index_col=None)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df.isna().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Warning: Dataset contains {missing_count} missing values. These will be handled in preprocessing.\")\n",
    "        # Simple imputation for missing values\n",
    "        df = df.fillna(df.mean())\n",
    "    \n",
    "    # Convert to numpy for preprocessing\n",
    "    data = df.to_numpy()\n",
    "    \n",
    "    # Split data into train and test\n",
    "    train_val_data, test_data = train_test_split(\n",
    "        data, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split train_val into train and validation\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_val_data,\n",
    "        test_size=val_ratio,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    val_data_scaled = scaler.transform(val_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_tensor = torch.tensor(train_data_scaled, dtype=torch.float32).to(device)\n",
    "    val_tensor = torch.tensor(val_data_scaled, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create column indices\n",
    "    column_indices = torch.arange(train_tensor.shape[1]).to(device)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of training samples: {train_tensor.shape[0]}\")\n",
    "    print(f\"Number of validation samples: {val_tensor.shape[0]}\")\n",
    "    print(f\"Number of test samples: {test_tensor.shape[0]}\")\n",
    "    print(f\"Number of features: {train_tensor.shape[1]}\")\n",
    "    \n",
    "    return train_tensor, val_tensor, test_tensor, scaler, column_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mnar_weighted_loss(predictions, ground_truth, mask, mechanism=None):\n",
    "    \"\"\"\n",
    "    Enhanced compute loss with adaptive weighting for MNAR and MAR scenarios,\n",
    "    with special focus on high missing percentage scenarios.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values\n",
    "        ground_truth (torch.Tensor): True values\n",
    "        mask (torch.Tensor): Binary mask (1 = missing, 0 = present)\n",
    "        mechanism (str): Missing data mechanism (MCAR, MAR, MNAR)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Weighted loss\n",
    "    \"\"\"\n",
    "    batch_size, num_features = ground_truth.shape\n",
    "    \n",
    "    # Use Huber loss for robustness to outliers - with beta adjusted\n",
    "    huber_loss = nn.SmoothL1Loss(reduction='none', beta=0.15)\n",
    "    element_loss = huber_loss(predictions, ground_truth)\n",
    "    \n",
    "    # Calculate missing percentage in this batch\n",
    "    batch_missing_pct = mask.float().mean().item()\n",
    "    \n",
    "    # Base mechanism-specific weighting with adaptive scaling based on missing percentage\n",
    "    if mechanism == \"MNAR\":\n",
    "        # Higher weight for MNAR as missingness increases\n",
    "        base_weight = 2.0 + batch_missing_pct * 2.0  # Scales from 2.0 at 0% to 4.0 at 100%\n",
    "    elif mechanism == \"MAR\":\n",
    "        # Significantly higher weight for MAR at high missing percentages\n",
    "        base_weight = 1.0 + batch_missing_pct * 3.0  # Scales from 1.0 at 0% to 4.0 at 100%\n",
    "    else:  # MCAR\n",
    "        # Slightly higher weight for MCAR at higher missing percentages\n",
    "        base_weight = 1.0 + batch_missing_pct * 0.5  # Scales from 1.0 at 0% to 1.5 at 100%\n",
    "    \n",
    "    # Common pattern detection - features that tend to be missing together\n",
    "    missing_per_feature = mask.float().mean(dim=0)\n",
    "    missing_correlation = torch.zeros_like(mask, dtype=torch.float32)\n",
    "    \n",
    "    # For MAR and MNAR, apply more sophisticated weighting\n",
    "    if mechanism in [\"MAR\", \"MNAR\"]:\n",
    "        # Get observed feature values\n",
    "        observed_mask = 1 - mask\n",
    "        \n",
    "        # Calculate feature statistics where observed\n",
    "        feature_means = torch.zeros_like(ground_truth)\n",
    "        feature_stds = torch.ones_like(ground_truth)\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            feat_observed = ground_truth[:, i] * observed_mask[:, i]\n",
    "            feat_count = observed_mask[:, i].sum() + 1e-8\n",
    "            \n",
    "            # Mean calculation\n",
    "            feat_mean = feat_observed.sum() / feat_count\n",
    "            feature_means[:, i] = feat_mean\n",
    "            \n",
    "            # Standard deviation calculation\n",
    "            if feat_count > 1:\n",
    "                feat_variance = ((feat_observed - feat_mean * observed_mask[:, i]) ** 2).sum() / feat_count\n",
    "                feat_std = torch.sqrt(feat_variance + 1e-8)\n",
    "                feature_stds[:, i] = feat_std\n",
    "        \n",
    "        # Calculate z-scores (deviation from mean, normalized by std)\n",
    "        z_scores = torch.abs(ground_truth - feature_means) / (feature_stds + 1e-8)\n",
    "        \n",
    "        # Clip to prevent extreme values\n",
    "        z_scores = torch.clamp(z_scores, 0, 3.5)\n",
    "        \n",
    "        if mechanism == \"MNAR\":\n",
    "            # For MNAR, higher deviation = higher weight (abnormal values more likely missing)\n",
    "            # Use sigmoid for smoother weighting, centered around z=1.5\n",
    "            value_weights = 1.0 + torch.sigmoid((z_scores - 1.5) * 2.0)\n",
    "            \n",
    "            # Final weights for MNAR combine base weight with value-dependent weights\n",
    "            weights = base_weight * mask * value_weights\n",
    "            \n",
    "        elif mechanism == \"MAR\":\n",
    "            # For MAR, detect feature interdependencies for better weighting\n",
    "            # Compute an interdependency score for each sample/feature\n",
    "            for i in range(num_features):\n",
    "                # Find correlated features (ones that tend to be missing together)\n",
    "                other_features_mask = mask[:, torch.arange(num_features) != i]\n",
    "                this_feature_mask = mask[:, i].unsqueeze(1).expand(-1, num_features-1)\n",
    "                \n",
    "                # Calculate correlation factor for this feature with others\n",
    "                correlation_factor = torch.mean((other_features_mask == this_feature_mask).float(), dim=1)\n",
    "                missing_correlation[:, i] = correlation_factor\n",
    "            \n",
    "            # Higher interdependency = higher weight (harder to predict)\n",
    "            # Scale to reasonable range\n",
    "            interdep_weights = 1.0 + missing_correlation\n",
    "            \n",
    "            # Add position-based weighting for MAR (later features often depend on earlier ones)\n",
    "            position_weight = torch.ones_like(mask, dtype=torch.float32, device=mask.device)\n",
    "            for i in range(num_features):\n",
    "                # Later features get progressively higher weights\n",
    "                position_weight[:, i] = 1.0 + (i / num_features) * 0.5\n",
    "            \n",
    "            # Final weights for MAR with interdependency and position weighting\n",
    "            weights = base_weight * mask * interdep_weights * position_weight\n",
    "            \n",
    "    else:  # MCAR\n",
    "        # For MCAR, weight by relative position and apply adaptive batch weighting\n",
    "        position_weight = torch.ones_like(mask, dtype=torch.float32, device=mask.device)\n",
    "        for i in range(num_features):\n",
    "            # Middle features get slightly higher weights (typically more informative)\n",
    "            rel_pos = abs(i - num_features/2) / (num_features/2)  # 0 at middle, 1 at edges\n",
    "            position_weight[:, i] = 1.0 + (1.0 - rel_pos) * 0.3  # Max 1.3 at middle\n",
    "        \n",
    "        weights = base_weight * mask * position_weight\n",
    "    \n",
    "    # Common adaptive weighting based on density of missingness\n",
    "    # Features with more missing values in this batch get higher weights\n",
    "    feature_missing_density = mask.float().mean(dim=0, keepdim=True)\n",
    "    density_weight = 1.0 + feature_missing_density * 0.5  # Up to 1.5x for fully missing features\n",
    "    \n",
    "    # Apply density weighting\n",
    "    weights = weights * density_weight\n",
    "    \n",
    "    # Apply loss focus - emphasize difficult samples\n",
    "    focused_loss = element_loss * (1.0 + element_loss)  # Give higher weight to samples with higher loss\n",
    "    \n",
    "    # Compute weighted average loss\n",
    "    weighted_loss = (focused_loss * weights).sum() / (weights.sum() + 1e-8)\n",
    "    \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask, mechanism=None):\n",
    "    \"\"\"\n",
    "    Compute loss using the enhanced MNAR-weighted loss function.\n",
    "    \"\"\"\n",
    "    return compute_mnar_weighted_loss(predictions, ground_truth, mask, mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error on masked positions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values\n",
    "        ground_truth (torch.Tensor): True values\n",
    "        mask (torch.Tensor): Binary mask (1 = missing, 0 = present)\n",
    "        \n",
    "    Returns:\n",
    "        float: RMSE value\n",
    "    \"\"\"\n",
    "    masked_preds = predictions[mask == 1]\n",
    "    masked_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    if len(masked_preds) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mse = torch.mean((masked_preds - masked_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Compute Normalized Root Mean Squared Error on masked positions.\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values\n",
    "        ground_truth (torch.Tensor): True values\n",
    "        mask (torch.Tensor): Binary mask (1 = missing, 0 = present)\n",
    "        \n",
    "    Returns:\n",
    "        float: NRMSE value\n",
    "    \"\"\"\n",
    "    masked_preds = predictions[mask == 1]\n",
    "    masked_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    if len(masked_preds) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mse = torch.mean((masked_preds - masked_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, missing_fraction, mechanisms=None, \n",
    "              mechanism_weights=None, scheduler=None, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Enhanced training for one epoch with mechanism-weighted sampling and curriculum learning.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to train\n",
    "        dataloader (DataLoader): Training data loader\n",
    "        optimizer (Optimizer): Optimizer\n",
    "        missing_fraction (float): Fraction of values to mask\n",
    "        mechanisms (list): List of missing data mechanisms to use\n",
    "        mechanism_weights (dict): Weights for sampling different mechanisms\n",
    "        scheduler (LRScheduler, optional): Learning rate scheduler\n",
    "        accumulation_steps (int): Number of steps to accumulate gradients\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    mechanism_losses = {m: 0.0 for m in mechanisms}\n",
    "    mechanism_counts = {m: 0 for m in mechanisms}\n",
    "    \n",
    "    # Default mechanism weights if not provided\n",
    "    if mechanism_weights is None:\n",
    "        mechanism_weights = {m: 1.0 / len(mechanisms) for m in mechanisms}\n",
    "    \n",
    "    # Normalize weights to probabilities\n",
    "    weight_sum = sum(mechanism_weights.values())\n",
    "    mechanism_probs = {m: w / weight_sum for m, w in mechanism_weights.items()}\n",
    "    \n",
    "    # Calculate epoch fraction for curriculum learning\n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    # Mixup probability - higher for early training\n",
    "    mixup_prob = 0.4\n",
    "    \n",
    "    if mechanisms is None:\n",
    "        mechanisms = [\"MCAR\"]\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    optimizer.zero_grad()  # Zero gradients at the beginning\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        x = batch[0].to(device)\n",
    "        \n",
    "        # Curriculum learning for missing fraction\n",
    "        # Gradually increase difficulty\n",
    "        epoch_progress = i / total_batches\n",
    "        if epoch_progress < 0.3:\n",
    "            # Start with easier task (less missing values)\n",
    "            curr_missing_fraction = missing_fraction * 0.6\n",
    "        elif epoch_progress < 0.6:\n",
    "            # Medium difficulty\n",
    "            curr_missing_fraction = missing_fraction * 0.8\n",
    "        else:\n",
    "            # Full difficulty\n",
    "            curr_missing_fraction = missing_fraction\n",
    "        \n",
    "        # Weighted sampling of mechanisms\n",
    "        mechanism_choices = list(mechanism_probs.keys())\n",
    "        mechanism_p = [mechanism_probs[m] for m in mechanism_choices]\n",
    "        mechanism = np.random.choice(mechanism_choices, p=mechanism_p)\n",
    "        \n",
    "        # Create a mask for missing values\n",
    "        mask = create_missing_mask(x, curr_missing_fraction, mechanism)\n",
    "        \n",
    "        # Create input with missing values set to 0\n",
    "        x_masked = x.clone()\n",
    "        x_masked[mask == 1] = 0\n",
    "        \n",
    "        # Apply mixup with probability\n",
    "        if np.random.random() < mixup_prob:\n",
    "            # Create shuffled indices\n",
    "            indices = torch.randperm(x.size(0), device=device)\n",
    "            \n",
    "            # Mix up samples with lambda drawn from beta distribution\n",
    "            lam = np.random.beta(0.2, 0.2)\n",
    "            \n",
    "            # Mix the data\n",
    "            mixed_x = lam * x_masked + (1 - lam) * x_masked[indices]\n",
    "            mixed_mask = mask | mask[indices]  # Union of masks\n",
    "            \n",
    "            # Forward pass with mixed data\n",
    "            column_indices = torch.arange(x.shape[1], device=device)\n",
    "            predictions = model(mixed_x, column_indices, mixed_mask)\n",
    "            \n",
    "            # Compute mixed loss\n",
    "            loss = lam * compute_loss(predictions, x, mask, mechanism) + \\\n",
    "                  (1 - lam) * compute_loss(predictions, x[indices], mask[indices], mechanism)\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            column_indices = torch.arange(x.shape[1], device=device)\n",
    "            predictions = model(x_masked, column_indices, mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(predictions, x, mask, mechanism)\n",
    "        \n",
    "        # Apply mechanism-specific weight adjustment to loss (beyond just sampling frequency)\n",
    "        mechanism_loss_weight = 1.0\n",
    "        if mechanism == \"MNAR\":\n",
    "            mechanism_loss_weight = 1.3  # Give MNAR errors more weight\n",
    "        elif mechanism == \"MAR\":\n",
    "            mechanism_loss_weight = 1.1  # Give MAR errors more weight\n",
    "            \n",
    "        loss = loss * mechanism_loss_weight\n",
    "        \n",
    "        # Track mechanism-specific losses\n",
    "        mechanism_losses[mechanism] += loss.item()\n",
    "        mechanism_counts[mechanism] += 1\n",
    "        \n",
    "        # Scale loss by accumulation steps   \n",
    "        loss = loss / accumulation_steps\n",
    "            \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Only step optimizer and scheduler after accumulation\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
    "            # Add gradient clipping (reduced for stability)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress bar\n",
    "        total_loss += loss.item() * accumulation_steps  # Scale back for reporting\n",
    "        \n",
    "        # Build mechanism-specific loss reporting\n",
    "        mech_loss_str = \" | \".join(\n",
    "            [f\"{m}: {mechanism_losses[m]/max(1, mechanism_counts[m]):.4f}\" \n",
    "             for m in mechanisms if mechanism_counts[m] > 0]\n",
    "        )\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": loss.item() * accumulation_steps,\n",
    "            \"missing\": f\"{curr_missing_fraction:.2f}\",\n",
    "            \"mech\": mechanism,\n",
    "            \"losses\": mech_loss_str\n",
    "        })\n",
    "    \n",
    "    # Calculate average loss overall and per mechanism\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    mech_avg_losses = {\n",
    "        m: mechanism_losses[m] / max(1, mechanism_counts[m])\n",
    "        for m in mechanisms\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"mechanism_losses\": mech_avg_losses,\n",
    "        \"mechanism_counts\": mechanism_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, missing_fraction, mechanisms=None):\n",
    "    \"\"\"\n",
    "    Validate the model on all mechanisms.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to validate\n",
    "        dataloader (DataLoader): Validation data loader\n",
    "        missing_fraction (float): Fraction of values to mask\n",
    "        mechanisms (list): List of missing data mechanisms to use\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with validation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_rmse = 0\n",
    "    total_nrmse = 0\n",
    "    \n",
    "    if mechanisms is None:\n",
    "        mechanisms = [\"MCAR\"]\n",
    "    \n",
    "    mechanism_metrics = {m: {\"loss\": 0, \"rmse\": 0, \"nrmse\": 0, \"count\": 0} for m in mechanisms}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            for mechanism in mechanisms:\n",
    "                # Create a mask for missing values\n",
    "                mask = create_missing_mask(x, missing_fraction, mechanism)\n",
    "                \n",
    "                # Create input with missing values set to 0\n",
    "                x_masked = x.clone()\n",
    "                x_masked[mask == 1] = 0\n",
    "                \n",
    "                # Forward pass\n",
    "                column_indices = torch.arange(x.shape[1], device=device)\n",
    "                predictions = model(x_masked, column_indices, mask)\n",
    "                \n",
    "                # Compute metrics\n",
    "                loss = compute_loss(predictions, x, mask)\n",
    "                rmse = compute_rmse(predictions, x, mask)\n",
    "                nrmse = compute_nrmse(predictions, x, mask)\n",
    "                \n",
    "                # Update mechanism-specific metrics\n",
    "                mechanism_metrics[mechanism][\"loss\"] += loss.item()\n",
    "                mechanism_metrics[mechanism][\"rmse\"] += rmse\n",
    "                mechanism_metrics[mechanism][\"nrmse\"] += nrmse\n",
    "                mechanism_metrics[mechanism][\"count\"] += 1\n",
    "                \n",
    "                # Update overall metrics\n",
    "                total_loss += loss.item()\n",
    "                total_rmse += rmse\n",
    "                total_nrmse += nrmse\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_evaluations = len(dataloader) * len(mechanisms)\n",
    "    avg_loss = total_loss / num_evaluations\n",
    "    avg_rmse = total_rmse / num_evaluations\n",
    "    avg_nrmse = total_nrmse / num_evaluations\n",
    "    \n",
    "    # Calculate mechanism-specific averages\n",
    "    for m in mechanisms:\n",
    "        if mechanism_metrics[m][\"count\"] > 0:\n",
    "            mechanism_metrics[m][\"loss\"] /= mechanism_metrics[m][\"count\"]\n",
    "            mechanism_metrics[m][\"rmse\"] /= mechanism_metrics[m][\"count\"]\n",
    "            mechanism_metrics[m][\"nrmse\"] /= mechanism_metrics[m][\"count\"]\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rmse\": avg_rmse,\n",
    "        \"nrmse\": avg_nrmse,\n",
    "        \"mechanisms\": mechanism_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training history.\n",
    "    \n",
    "    Args:\n",
    "        history (dict): Training history\n",
    "        save_path (str, optional): Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot NRMSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history[\"val_nrmse\"], label=\"Overall NRMSE\")\n",
    "    plt.plot(history[\"val_mcar_nrmse\"], label=\"MCAR NRMSE\")\n",
    "    plt.plot(history[\"val_mar_nrmse\"], label=\"MAR NRMSE\")\n",
    "    plt.plot(history[\"val_mnar_nrmse\"], label=\"MNAR NRMSE\")\n",
    "    plt.title(\"NRMSE by Mechanism\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"NRMSE\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot learning rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history[\"lr\"])\n",
    "    plt.title(\"Learning Rate\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, config, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Save model and related objects.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        scaler (StandardScaler): Data scaler\n",
    "        config (dict): Model configuration\n",
    "        save_dir (str): Directory to save model\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(save_dir, \"tabular_transformer_relpos.pth\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config\n",
    "    }, model_path)\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = os.path.join(save_dir, \"scaler.pkl\")\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Scaler saved to {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, column_indices, config, model_type=\"single\"):\n",
    "    \"\"\"\n",
    "    Train the model (single model or ensemble).\n",
    "    \n",
    "    Args:\n",
    "        train_data (torch.Tensor): Training data\n",
    "        val_data (torch.Tensor): Validation data\n",
    "        column_indices (torch.Tensor): Column indices\n",
    "        config (dict): Configuration dictionary\n",
    "        model_type (str): Type of model to train (\"single\" or \"ensemble\")\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Trained model and training history\n",
    "    \"\"\"\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = TensorDataset(train_data)\n",
    "    val_dataset = TensorDataset(val_data)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config[\"batch_size\"], \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Create model based on type\n",
    "    num_features = train_data.shape[1]\n",
    "    \n",
    "    if model_type == \"ensemble\":\n",
    "        print(\"Creating ensemble model with 3 base models...\")\n",
    "        model = EnsembleModel(\n",
    "            num_features=num_features,\n",
    "            config=config,\n",
    "            num_models=3\n",
    "        ).to(device)\n",
    "    else:\n",
    "        print(\"Creating single transformer model...\")\n",
    "        model = TabularTransformerWithRelPos(\n",
    "            num_features=num_features,\n",
    "            d_model=config[\"d_model\"],\n",
    "            nhead=config[\"num_heads\"],\n",
    "            num_layers=config[\"num_layers\"],\n",
    "            dim_feedforward=config[\"dim_feedforward\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            activation=config[\"activation\"],\n",
    "            max_seq_len=max(2 * num_features, 100)  # Set max_seq_len based on feature count\n",
    "        ).to(device)\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"learning_rate\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    accumulation_steps = config.get(\"accumulation_steps\", 4)\n",
    "    \n",
    "    # Learning rate scheduler - use OneCycleLR for better convergence\n",
    "    total_steps = (len(train_dataloader) // accumulation_steps) * config[\"num_epochs\"]\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config[\"learning_rate\"],\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.1,  # Shorter warm-up phase\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=10.0,  # Less aggressive initial LR reduction\n",
    "        final_div_factor=1000.0  # Less aggressive final LR\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_rmse\": [],\n",
    "        \"val_nrmse\": [],\n",
    "        \"val_mcar_nrmse\": [],\n",
    "        \"val_mar_nrmse\": [],\n",
    "        \"val_mnar_nrmse\": [],\n",
    "        \"lr\": []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    \n",
    "    # Get mechanism weights from config if available\n",
    "    mechanism_weights = config.get(\"mechanisms_weights\", None)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            config[\"missing_fraction\"],\n",
    "            mechanisms=mechanisms,\n",
    "            mechanism_weights=mechanism_weights,\n",
    "            scheduler=scheduler,\n",
    "            accumulation_steps=config.get(\"accumulation_steps\", 4)\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            config[\"missing_fraction\"],\n",
    "            mechanisms\n",
    "        )\n",
    "        \n",
    "        # Update history\n",
    "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
    "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "        history[\"val_rmse\"].append(val_metrics[\"rmse\"])\n",
    "        history[\"val_nrmse\"].append(val_metrics[\"nrmse\"])\n",
    "        history[\"val_mcar_nrmse\"].append(val_metrics[\"mechanisms\"][\"MCAR\"][\"nrmse\"])\n",
    "        history[\"val_mar_nrmse\"].append(val_metrics[\"mechanisms\"][\"MAR\"][\"nrmse\"])\n",
    "        history[\"val_mnar_nrmse\"].append(val_metrics[\"mechanisms\"][\"MNAR\"][\"nrmse\"])\n",
    "        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Val NRMSE: {val_metrics['nrmse']:.4f}\")\n",
    "        print(f\"Val MCAR NRMSE: {val_metrics['mechanisms']['MCAR']['nrmse']:.4f}\")\n",
    "        print(f\"Val MAR NRMSE: {val_metrics['mechanisms']['MAR']['nrmse']:.4f}\")\n",
    "        print(f\"Val MNAR NRMSE: {val_metrics['mechanisms']['MNAR']['nrmse']:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_metrics[\"loss\"] < best_val_loss:\n",
    "            best_val_loss = val_metrics[\"loss\"]\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config[\"patience\"]:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(data, column_indices, config, n_folds=5, model_type=\"single\"):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        data (torch.Tensor): Data to split\n",
    "        column_indices (torch.Tensor): Feature column indices\n",
    "        config (dict): Configuration dictionary\n",
    "        n_folds (int): Number of folds\n",
    "        model_type (str): Type of model to train (\"single\" or \"ensemble\")\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with average metrics across folds\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    all_metrics = []\n",
    "    all_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{n_folds}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_data = data[train_idx]\n",
    "        val_data = data[val_idx]\n",
    "        \n",
    "        # Train model (can be single or ensemble based on model_type)\n",
    "        model, history = train_model(train_data, val_data, column_indices, config, model_type=model_type)\n",
    "        \n",
    "        # Store model\n",
    "        all_models.append(model)\n",
    "        \n",
    "        # Validate\n",
    "        val_dataloader = DataLoader(TensorDataset(val_data), batch_size=config[\"batch_size\"])\n",
    "        metrics = validate(model, val_dataloader, config[\"missing_fraction\"], [\"MCAR\", \"MAR\", \"MNAR\"])\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"Fold {fold+1} metrics:\")\n",
    "        print(f\"  NRMSE: {metrics['nrmse']:.4f}\")\n",
    "        print(f\"  MCAR NRMSE: {metrics['mechanisms']['MCAR']['nrmse']:.4f}\")\n",
    "        print(f\"  MAR NRMSE: {metrics['mechanisms']['MAR']['nrmse']:.4f}\")\n",
    "        print(f\"  MNAR NRMSE: {metrics['mechanisms']['MNAR']['nrmse']:.4f}\")\n",
    "    \n",
    "    # Average metrics across folds\n",
    "    avg_metrics = {\n",
    "        \"nrmse\": np.mean([m[\"nrmse\"] for m in all_metrics]),\n",
    "        \"mcar_nrmse\": np.mean([m[\"mechanisms\"][\"MCAR\"][\"nrmse\"] for m in all_metrics]),\n",
    "        \"mar_nrmse\": np.mean([m[\"mechanisms\"][\"MAR\"][\"nrmse\"] for m in all_metrics]),\n",
    "        \"mnar_nrmse\": np.mean([m[\"mechanisms\"][\"MNAR\"][\"nrmse\"] for m in all_metrics])\n",
    "    }\n",
    "    \n",
    "    # Standard deviation of metrics\n",
    "    std_metrics = {\n",
    "        \"nrmse_std\": np.std([m[\"nrmse\"] for m in all_metrics]),\n",
    "        \"mcar_nrmse_std\": np.std([m[\"mechanisms\"][\"MCAR\"][\"nrmse\"] for m in all_metrics]),\n",
    "        \"mar_nrmse_std\": np.std([m[\"mechanisms\"][\"MAR\"][\"nrmse\"] for m in all_metrics]),\n",
    "        \"mnar_nrmse_std\": np.std([m[\"mechanisms\"][\"MNAR\"][\"nrmse\"] for m in all_metrics])\n",
    "    }\n",
    "    \n",
    "    # Add standard deviations to results\n",
    "    avg_metrics.update(std_metrics)\n",
    "    \n",
    "    print(\"\\nAverage metrics across folds:\")\n",
    "    print(f\"  NRMSE: {avg_metrics['nrmse']:.4f}  {avg_metrics['nrmse_std']:.4f}\")\n",
    "    print(f\"  MCAR NRMSE: {avg_metrics['mcar_nrmse']:.4f}  {avg_metrics['mcar_nrmse_std']:.4f}\")\n",
    "    print(f\"  MAR NRMSE: {avg_metrics['mar_nrmse']:.4f}  {avg_metrics['mar_nrmse_std']:.4f}\")\n",
    "    print(f\"  MNAR NRMSE: {avg_metrics['mnar_nrmse']:.4f}  {avg_metrics['mnar_nrmse_std']:.4f}\")\n",
    "    \n",
    "    return avg_metrics, all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution code\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    # Optimized configuration for better NRMSE\n",
    "    config = {\n",
    "        # Data parameters\n",
    "        \"data_path\": \"./data/physionet_39_features_only_no_leakage.csv\",\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.1,\n",
    "        \n",
    "        # Model parameters - Balanced for overall performance \n",
    "        \"d_model\": 320,      # Increased from 256, but still less than original 384\n",
    "        \"num_heads\": 10,     # Increased from 8, but less than original 12  \n",
    "        \"num_layers\": 4,     # Reduced from 5\n",
    "        \"dim_feedforward\": 1280,  # Increased from 1024\n",
    "        \"dropout\": 0.18,     # Slightly decreased from 0.2\n",
    "        \"activation\": \"gelu\",\n",
    "        \n",
    "        # Training parameters - Optimized for better generalization\n",
    "        \"batch_size\": 48,    # Adjusted from 64\n",
    "        \"learning_rate\": 0.00018,  # Slightly decreased\n",
    "        \"weight_decay\": 0.012,    # Adjusted from 0.015\n",
    "        \"num_epochs\": 100,   # Increased from 80\n",
    "        \"patience\": 20,      # Increased from 15\n",
    "        \"missing_fraction\": 0.4,  # Increased from 0.35 to focus more on higher missingness\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        \"warmup_steps_pct\": 0.07,  # Increased from 0.05\n",
    "        \"min_lr_factor\": 200.0,    # More gradual decay\n",
    "        \"accumulation_steps\": 2,   # Added gradient accumulation for more stable updates\n",
    "        \n",
    "        # Save parameters\n",
    "        \"save_dir\": \"models\"\n",
    "    }\n",
    "    \n",
    "    # Create directories with timestamp\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    save_dir = os.path.join(config[\"save_dir\"], f\"{current_time}_model-single\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create experiment log file\n",
    "    experiment_log = os.path.join(save_dir, \"experiment_log.txt\")\n",
    "    with open(experiment_log, \"w\") as f:\n",
    "        f.write(f\"Experiment started at: {current_time}\\n\")\n",
    "        f.write(\"Configuration:\\n\")\n",
    "        for key, value in config.items():\n",
    "            f.write(f\"  {key}: {value}\\n\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    train_data, val_data, test_data, scaler, column_indices = load_and_prepare_data(\n",
    "        config[\"data_path\"],\n",
    "        config[\"test_size\"],\n",
    "        config[\"val_size\"]\n",
    "    )\n",
    "    \n",
    "    # Set flag to skip k-fold validation\n",
    "    RUN_KFOLD = False\n",
    "    \n",
    "    # Train final model directly (skip K-fold)\n",
    "    print(\"\\n=== Training Model ===\")\n",
    "    model, history = train_model(train_data, val_data, column_indices, config, model_type=\"single\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, os.path.join(save_dir, \"training_history.png\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n=== Evaluating on Test Set ===\")\n",
    "    test_dataloader = DataLoader(TensorDataset(test_data), batch_size=config[\"batch_size\"])\n",
    "    test_metrics = validate(model, test_dataloader, config[\"missing_fraction\"], [\"MCAR\", \"MAR\", \"MNAR\"])\n",
    "    \n",
    "    print(f\"Test metrics:\")\n",
    "    print(f\"  Overall NRMSE: {test_metrics['nrmse']:.4f}\")\n",
    "    print(f\"  MCAR NRMSE: {test_metrics['mechanisms']['MCAR']['nrmse']:.4f}\")\n",
    "    print(f\"  MAR NRMSE: {test_metrics['mechanisms']['MAR']['nrmse']:.4f}\")\n",
    "    print(f\"  MNAR NRMSE: {test_metrics['mechanisms']['MNAR']['nrmse']:.4f}\")\n",
    "    \n",
    "    # Save test results\n",
    "    test_results_path = os.path.join(save_dir, \"test_results.txt\")\n",
    "    with open(test_results_path, \"w\") as f:\n",
    "        f.write(\"Test Results:\\n\")\n",
    "        f.write(f\"  Overall NRMSE: {test_metrics['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MCAR NRMSE: {test_metrics['mechanisms']['MCAR']['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MAR NRMSE: {test_metrics['mechanisms']['MAR']['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MNAR NRMSE: {test_metrics['mechanisms']['MNAR']['nrmse']:.4f}\\n\")\n",
    "    \n",
    "    # Save models\n",
    "    print(f\"\\n=== Saving Models ===\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_save_path = os.path.join(save_dir, \"final_model.pth\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"config\": config,\n",
    "        \"model_type\": \"single\"\n",
    "    }, model_save_path)\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = os.path.join(save_dir, \"scaler.pkl\")\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    # Update experiment log\n",
    "    with open(experiment_log, \"a\") as f:\n",
    "        f.write(\"\\nFinal Test Results:\\n\")\n",
    "        f.write(f\"  Overall NRMSE: {test_metrics['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MCAR NRMSE: {test_metrics['mechanisms']['MCAR']['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MAR NRMSE: {test_metrics['mechanisms']['MAR']['nrmse']:.4f}\\n\")\n",
    "        f.write(f\"  MNAR NRMSE: {test_metrics['mechanisms']['MNAR']['nrmse']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nExperiment completed at: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "    print(f\"Training complete! All models and results saved in: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imputation_performance(model, test_data, column_indices, missing_percentages=[0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                                    mechanisms=[\"MCAR\", \"MAR\", \"MNAR\"]):\n",
    "    \"\"\"\n",
    "    Evaluate the model's imputation performance across different missing percentages and mechanisms.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model\n",
    "        test_data (torch.Tensor): Test data tensor\n",
    "        column_indices (torch.Tensor): Column indices\n",
    "        missing_percentages (list): List of missing data percentages to evaluate\n",
    "        mechanisms (list): List of missing data mechanisms to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with NRMSE results for each mechanism and percentage\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for mechanism in mechanisms:\n",
    "            mechanism_results = []\n",
    "            \n",
    "            for missing_pct in missing_percentages:\n",
    "                batch_size = 128\n",
    "                total_nrmse = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                # Create DataLoader for test data\n",
    "                test_dataloader = DataLoader(TensorDataset(test_data), batch_size=batch_size)\n",
    "                \n",
    "                for batch in tqdm(test_dataloader, desc=f\"Evaluating {mechanism} at {missing_pct*100}%\"):\n",
    "                    x = batch[0].to(device)\n",
    "                    \n",
    "                    # Create a mask for missing values\n",
    "                    mask = create_missing_mask(x, missing_pct, mechanism)\n",
    "                    \n",
    "                    # Create input with missing values set to 0\n",
    "                    x_masked = x.clone()\n",
    "                    x_masked[mask == 1] = 0\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    predictions = model(x_masked, column_indices, mask)\n",
    "                    \n",
    "                    # Compute NRMSE\n",
    "                    nrmse = compute_nrmse(predictions, x, mask)\n",
    "                    total_nrmse += nrmse\n",
    "                    num_batches += 1\n",
    "                \n",
    "                avg_nrmse = total_nrmse / num_batches\n",
    "                mechanism_results.append(avg_nrmse)\n",
    "                print(f\"{mechanism} at {missing_pct*100}% missing: NRMSE = {avg_nrmse:.4f}\")\n",
    "            \n",
    "            results[mechanism] = mechanism_results\n",
    "    \n",
    "    # Create a DataFrame for visualization\n",
    "    results_df = pd.DataFrame(results, index=[f\"{int(pct*100)}%\" for pct in missing_percentages])\n",
    "    results_df.index.name = \"Missing Percentage\"\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_imputation_performance(results_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for imputation performance.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame with NRMSE results\n",
    "        save_path (str, optional): Path to save the figures\n",
    "    \"\"\"\n",
    "    # 1. Create a heatmap visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a heatmap with custom colormap (lower values = better = greener)\n",
    "    sns.heatmap(results_df, annot=True, cmap=\"RdYlGn_r\", fmt=\".4f\", \n",
    "                linewidths=.5, cbar_kws={'label': 'NRMSE (lower is better)'})\n",
    "    \n",
    "    plt.title(\"Imputation Performance (NRMSE) by Missing Mechanism and Percentage\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        heatmap_path = save_path.replace(\".png\", \"_heatmap.png\")\n",
    "        plt.savefig(heatmap_path)\n",
    "        print(f\"Heatmap saved to {heatmap_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Create a line plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Convert index to numeric for plotting\n",
    "    results_df_plot = results_df.copy()\n",
    "    results_df_plot.index = [int(idx.replace(\"%\", \"\")) for idx in results_df_plot.index]\n",
    "    \n",
    "    # Plot lines for each mechanism\n",
    "    for column in results_df_plot.columns:\n",
    "        plt.plot(results_df_plot.index, results_df_plot[column], marker='o', linewidth=2, label=column)\n",
    "    \n",
    "    plt.xlabel(\"Missing Percentage (%)\")\n",
    "    plt.ylabel(\"NRMSE (lower is better)\")\n",
    "    plt.title(\"Imputation Performance Across Missing Percentages\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.xticks(results_df_plot.index)\n",
    "    \n",
    "    if save_path:\n",
    "        line_path = save_path.replace(\".png\", \"_lineplot.png\")\n",
    "        plt.savefig(line_path)\n",
    "        print(f\"Line plot saved to {line_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Create a bar chart comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot grouped bars\n",
    "    bar_width = 0.25\n",
    "    r = np.arange(len(results_df_plot.index))\n",
    "    \n",
    "    # Plot bars for each mechanism\n",
    "    for i, column in enumerate(results_df_plot.columns):\n",
    "        plt.bar(r + i*bar_width, results_df_plot[column], width=bar_width, label=column)\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel(\"Missing Percentage (%)\")\n",
    "    plt.ylabel(\"NRMSE (lower is better)\")\n",
    "    plt.title(\"Imputation Performance by Missing Mechanism and Percentage\")\n",
    "    plt.xticks(r + bar_width, results_df_plot.index)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_path:\n",
    "        bar_path = save_path.replace(\".png\", \"_barchart.png\")\n",
    "        plt.savefig(bar_path)\n",
    "        print(f\"Bar chart saved to {bar_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Create a table visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    # Create a table with colored cells based on values\n",
    "    # First, normalize the data for coloring\n",
    "    norm_data = results_df.copy()\n",
    "    for col in norm_data.columns:\n",
    "        max_val = norm_data[col].max()\n",
    "        min_val = norm_data[col].min()\n",
    "        if max_val > min_val:\n",
    "            norm_data[col] = (norm_data[col] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            norm_data[col] = 0\n",
    "    \n",
    "    # Create a table with cell colors\n",
    "    cell_colors = plt.cm.RdYlGn_r(norm_data.values)\n",
    "    table = ax.table(cellText=results_df.values.round(4), \n",
    "                    rowLabels=results_df.index,\n",
    "                    colLabels=results_df.columns,\n",
    "                    cellColours=cell_colors,\n",
    "                    loc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    plt.title(\"NRMSE Values by Missing Mechanism and Percentage\", y=0.8)\n",
    "    \n",
    "    if save_path:\n",
    "        table_path = save_path.replace(\".png\", \"_table.png\")\n",
    "        plt.savefig(table_path, bbox_inches='tight')\n",
    "        print(f\"Table visualization saved to {table_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create a section for imputation performance evaluation\n",
    "    print(\"\\n=== Evaluating Imputation Performance Across Missing Percentages ===\")\n",
    "    \n",
    "    # Define the missing percentages to evaluate\n",
    "    missing_percentages = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    \n",
    "    # Create a directory for imputation performance visualizations\n",
    "    imputation_dir = os.path.join(save_dir, \"imputation_performance\")\n",
    "    os.makedirs(imputation_dir, exist_ok=True)\n",
    "    \n",
    "    # Evaluate imputation performance\n",
    "    results_df = evaluate_imputation_performance(\n",
    "        model,                    # Use the final trained model\n",
    "        test_data,                # Test data\n",
    "        column_indices,           # Column indices\n",
    "        missing_percentages,      # Missing percentages\n",
    "        mechanisms                # Missing mechanisms\n",
    "    )\n",
    "    \n",
    "    # Save the raw results to CSV\n",
    "    results_csv_path = os.path.join(imputation_dir, \"imputation_results.csv\")\n",
    "    results_df.to_csv(results_csv_path)\n",
    "    print(f\"Raw imputation results saved to {results_csv_path}\")\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    visualize_imputation_performance(\n",
    "        results_df,\n",
    "        save_path=os.path.join(imputation_dir, \"imputation_performance.png\")\n",
    "    )\n",
    "        \n",
    "    print(\"\\nImputation performance evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_original_dataset(trained_model, scaler, save_path=\"imputed_original.csv\"):\n",
    "    \"\"\"\n",
    "    Load the original CSV dataset, impute missing values using the pre-trained transformer model,\n",
    "    and save the imputed dataset.\n",
    "    \n",
    "    Args:\n",
    "        trained_model: The pre-trained imputation model\n",
    "        scaler: The fitted scaler used during model training\n",
    "        save_path: Path to save the imputed dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Imputing Original Dataset ===\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    trained_model.eval()\n",
    "    \n",
    "    # File path\n",
    "    original_file_path = \"./data/physionet_39_features_only_no_leakage.csv\"\n",
    "    \n",
    "    # Load the original dataset\n",
    "    print(f\"Loading original dataset from {original_file_path}...\")\n",
    "    df_original = pd.read_csv(original_file_path, index_col=None)\n",
    "    print(f\"Original dataset shape: {df_original.shape}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df_original.isna().sum().sum()\n",
    "    missing_percentage = (missing_count / (df_original.shape[0] * df_original.shape[1])) * 100\n",
    "    print(f\"Dataset contains {missing_count} missing values ({missing_percentage:.2f}% of all values)\")\n",
    "    \n",
    "    # Create a copy of the original dataset for imputation\n",
    "    df_imputed = df_original.copy()\n",
    "    \n",
    "    # Extract numerical columns for imputation\n",
    "    numerical_cols = df_original.select_dtypes(include=['number']).columns\n",
    "    print(f\"Found {len(numerical_cols)} numerical columns\")\n",
    "    \n",
    "    # Create mask for missing values (True where values are missing)\n",
    "    missing_mask = df_original[numerical_cols].isna()\n",
    "    \n",
    "    # Fill missing values with 0 for initial processing\n",
    "    df_filled = df_original[numerical_cols].fillna(0)\n",
    "    \n",
    "    # Scale the data using the provided scaler\n",
    "    data_scaled = scaler.transform(df_filled)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    data_tensor = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create column indices tensor\n",
    "    column_indices = torch.arange(data_tensor.shape[1]).to(device)\n",
    "    \n",
    "    # Create mask tensor (1 where values are missing, 0 otherwise)\n",
    "    mask_tensor = torch.tensor(missing_mask.values, dtype=torch.int).to(device)\n",
    "    \n",
    "    print(\"Performing imputation with trained model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get predictions from the model\n",
    "        imputed_tensor = trained_model(data_tensor, column_indices, mask_tensor)\n",
    "        \n",
    "        # Convert to numpy for processing\n",
    "        imputed_np = imputed_tensor.cpu().numpy()\n",
    "        \n",
    "        # Inverse transform to original scale\n",
    "        imputed_np = scaler.inverse_transform(imputed_np)\n",
    "        \n",
    "        # Create a DataFrame from the imputed values\n",
    "        imputed_df = pd.DataFrame(imputed_np, columns=numerical_cols, index=df_original.index)\n",
    "        \n",
    "        # Replace missing values in the original dataframe with imputed values\n",
    "        for col in numerical_cols:\n",
    "            if col in df_imputed.columns:\n",
    "                missing_idx = df_imputed[col].isna()\n",
    "                df_imputed.loc[missing_idx, col] = imputed_df.loc[missing_idx, col]\n",
    "    \n",
    "    # Save the imputed dataset\n",
    "    print(f\"Saving imputed dataset to {save_path}...\")\n",
    "    df_imputed.to_csv(save_path)\n",
    "    \n",
    "    # Verification\n",
    "    missing_after = df_imputed[numerical_cols].isna().sum().sum()\n",
    "    print(f\"Missing values in numerical columns after imputation: {missing_after}\")\n",
    "    \n",
    "    total_missing_after = df_imputed.isna().sum().sum()\n",
    "    if total_missing_after > 0:\n",
    "        print(f\"Total missing values after imputation: {total_missing_after}\")\n",
    "        print(\"Note: Non-numerical columns may still contain missing values\")\n",
    "    else:\n",
    "        print(\"All missing values have been successfully imputed\")\n",
    "    \n",
    "    print(\"Imputation complete!\")\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the imputation process\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=== Running Imputation on Original Dataset ===\")\n",
    "    \n",
    "    # Use the already trained model\n",
    "    # Assuming 'model' and 'scaler' are the trained model and scaler from earlier in the notebook\n",
    "    imputed_data = impute_original_dataset(model, scaler, \"imputed_original.csv\")\n",
    "    \n",
    "    # Display sample of the imputed data\n",
    "    print(\"\\nSample of imputed data:\")\n",
    "    print(imputed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
